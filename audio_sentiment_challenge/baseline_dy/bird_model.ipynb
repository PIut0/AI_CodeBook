{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1c95c7-5c2c-43c2-98d7-f6b7389fd012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import ast\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a9842-ab80-4271-83ff-36ef91576512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b170b79-e673-4e3f-af8f-d3053afc86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    seed=3\n",
    "    sample_rate= 16000\n",
    "    n_fft=1024\n",
    "    hop_length=512\n",
    "    n_mels=64\n",
    "    duration=5\n",
    "    num_classes = 6\n",
    "    train_batch_size = 32\n",
    "    valid_batch_size = 64\n",
    "    model_name = 'resnet50'\n",
    "    epochs = 20\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540ab98b-6096-42c6-bb30-baa32bf8905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99898fdb-bb5b-49bc-bdab-7d4902b0e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=config.sample_rate, \n",
    "                                                      n_fft=config.n_fft, \n",
    "                                                      hop_length=config.hop_length, \n",
    "                                                      n_mels=config.n_mels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73209b21-8585-42f0-a80c-13753ae4cf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5001.000000\n",
       "mean        2.454509\n",
       "std         1.714637\n",
       "min         0.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         4.000000\n",
       "max         5.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/scratch/network/mk8574/audio_sentiment_challenge/data/train.csv\")\n",
    "df[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2644c35e-575f-4925-9a3c-1fa575f08ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSentDataset(Dataset):\n",
    "    def __init__(self, df, transformation, target_sample_rate, duration ,mode='train'):\n",
    "        self.audio_paths = df['path'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = target_sample_rate*duration\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        audio_path = f'/scratch/network/mk8574/audio_sentiment_challenge/data/{self.audio_paths[index]}'\n",
    "        \n",
    "        signal, sr = torchaudio.load(audio_path) # loaded the audio\n",
    "        \n",
    "        # Now we first checked if the sample rate is same as TARGET_SAMPLE_RATE and if it not equal we perform resampling\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        \n",
    "        # Next we check the number of channels of the signal\n",
    "        #signal -> (num_channels, num_samples) - Eg.-(2, 14000) -> (1, 14000)\n",
    "        if signal.shape[0]>1:\n",
    "            signal = torch.mean(signal, axis=0, keepdim=True)\n",
    "        \n",
    "        # Lastly we check the number of samples of the signal\n",
    "        #signal -> (num_channels, num_samples) - Eg.-(1, 14000) -> (1, self.num_samples)\n",
    "        # If it is more than the required number of samples, we truncate the signal\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        \n",
    "        # If it is less than the required number of samples, we pad the signal\n",
    "        if signal.shape[1]<self.num_samples:\n",
    "            num_missing_samples = self.num_samples - signal.shape[1]\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = F.pad(signal, last_dim_padding)\n",
    "        \n",
    "        # Finally all the process has been done and now we will extract mel spectrogram from the signal\n",
    "        mel = self.transformation(signal)\n",
    "        \n",
    "        # For pretrained models, we need 3 channel image, so for that we concatenate the extracted mel\n",
    "        image = torch.cat([mel, mel, mel])\n",
    "        \n",
    "        # Normalized the image(그냥 나누기)\n",
    "        max_val = torch.abs(image).max()\n",
    "        image = abs(image) / max_val\n",
    "        \n",
    "        label = torch.tensor(self.labels[index])\n",
    "        if self.mode in [\"train\",\"valid\"]:\n",
    "            return image, label\n",
    "        elif self.mode in [\"test\"]:\n",
    "            return image\n",
    "        else:\n",
    "            assert False, f\"Invalid mode : {self.mode}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "232ef6f1-d226-4069-93bb-a487333a633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    train_df,valid_df = train_test_split(df, test_size=0.2, random_state=config.seed, shuffle=True)\n",
    "    \n",
    "    train_dataset = AudioSentDataset(train_df, mel_spectrogram, config.sample_rate, config.duration,mode = \"train\")\n",
    "    valid_dataset = AudioSentDataset(valid_df, mel_spectrogram, config.sample_rate, config.duration,mode = \"valid\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=config.valid_batch_size, shuffle=False,drop_last=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ffc4e9c3-819e-4f6c-b772-984242cf5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEFResnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BirdCLEFResnet, self).__init__()\n",
    "        self.base_model = models.__getattribute__(config.model_name)(pretrained=True)#True\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        in_features = self.base_model.fc.in_features\n",
    "        \n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, config.num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x.clamp(min=1e-8)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e41145d-5dc6-4cd3-a077-0ef98d723571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    return nn.CrossEntropyLoss(label_smoothing=0.001)(outputs, labels)\n",
    "\n",
    "def train(model, data_loader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    loop = tqdm(data_loader, position=0)\n",
    "    for i, (mels, labels) in enumerate(loop):\n",
    "        mels = mels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(mels)\n",
    "       \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        #rint(loss.item(),running_loss,end=\"-\")\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{config.epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "   \n",
    "    return running_loss/len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae6c8a4b-cc20-44fb-8f4b-fc15e0aacdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, data_loader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    \n",
    "    loop = tqdm(data_loader, position=0)\n",
    "    for mels, labels in loop:\n",
    "        mels = mels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(mels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        pred.extend(preds.view(-1).cpu().detach().numpy())\n",
    "        label.extend(labels.view(-1).cpu().detach().numpy())\n",
    "        \n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{config.epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    #valid_f1 = f1_score(label, pred, average='macro')\n",
    "    accuracy = sum(1 for x,y in zip(label,pred) if x == y) / len(pred)\n",
    "    return running_loss/len(data_loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c142172b-1fbe-40bc-9ecb-43843d8f40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def run():\n",
    "    now = datetime.datetime.now()\n",
    "    nowDatetime = now.strftime('%Y%m%d%H%M%S')\n",
    "    train_loader, valid_loader = get_data()\n",
    "    \n",
    "    #model = BirdClefModel().to(config.device) # check version 3 for this\n",
    "    model = BirdCLEFResnet().to(config.device)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=10)\n",
    "    \n",
    "    best_valid_f1 = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, scheduler, config.device, epoch)\n",
    "        valid_loss, valid_f1 = valid(model, valid_loader, config.device, epoch)\n",
    "        print(train_loss,valid_loss)\n",
    "        if valid_f1 > best_valid_f1:\n",
    "            print(f\"Validation Accuracy Improved - {best_valid_f1} ---> {valid_f1}\")\n",
    "            torch.save(model.state_dict(), f'./model_{nowDatetime}.bin')\n",
    "            print(f\"Saved model checkpoint at ./model_{nowDatetime+str(epoch)}.bin\")\n",
    "            best_valid_f1 = valid_f1\n",
    "        else:\n",
    "            print(f\"Validation Score : BEST: {best_valid_f1} NOW: {valid_f1}\")\n",
    "            \n",
    "    return best_valid_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "35f1ad26-40ef-4a17-bec5-49a96597c37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]:   6%|▋         | 8/125 [00:03<00:57,  2.03it/s, loss=1.8] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_valid_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest ACCURACY: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_valid_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[0;32mIn[68], line 16\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m best_valid_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m---> 16\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     valid_loss, valid_f1 \u001b[38;5;241m=\u001b[39m valid(model, valid_loader, config\u001b[38;5;241m.\u001b[39mdevice, epoch)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(train_loss,valid_loss)\n",
      "Cell \u001b[0;32mIn[67], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, scheduler, device, epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(data_loader, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (mels, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loop):\n\u001b[1;32m     10\u001b[0m     mels \u001b[38;5;241m=\u001b[39m mels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[64], line 39\u001b[0m, in \u001b[0;36mAudioSentDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     37\u001b[0m     num_missing_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m-\u001b[39m signal\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     38\u001b[0m     last_dim_padding \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, num_missing_samples)\n\u001b[0;32m---> 39\u001b[0m     signal \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_padding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Finally all the process has been done and now we will extract mel spectrogram from the signal\u001b[39;00m\n\u001b[1;32m     42\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation(signal)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "best_valid_f1 = run()\n",
    "print(f'Best ACCURACY: {best_valid_f1:.5f}')\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()    \n",
    " # To run for all the folds, just remove this break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15106178-7e5f-4bd3-ac4d-d78b98601af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk8574_3.10 [~/.conda/envs/mk8574_3.10/]",
   "language": "python",
   "name": "conda_mk8574_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
