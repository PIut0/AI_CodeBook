{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1c95c7-5c2c-43c2-98d7-f6b7389fd012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import ast\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b170b79-e673-4e3f-af8f-d3053afc86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    seed = 42\n",
    "    num_fold = 1\n",
    "    sample_rate = 16000\n",
    "    n_fft = 1024\n",
    "    hop_length = 512\n",
    "    n_mels = 64\n",
    "    duration = 5\n",
    "    num_classes = 6\n",
    "    train_batch_size = 16\n",
    "    valid_batch_size = 16\n",
    "    model_name = 'swin_v2_s'\n",
    "    epochs = 50\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540ab98b-6096-42c6-bb30-baa32bf8905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73209b21-8585-42f0-a80c-13753ae4cf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                    path  label\n",
       "0  TRAIN_0000  ./train/TRAIN_0000.wav      1\n",
       "1  TRAIN_0001  ./train/TRAIN_0001.wav      2\n",
       "2  TRAIN_0002  ./train/TRAIN_0002.wav      4\n",
       "3  TRAIN_0003  ./train/TRAIN_0003.wav      5\n",
       "4  TRAIN_0004  ./train/TRAIN_0004.wav      4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/scratch/network/mk8574/audio_sentiment_challenge/data/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d772e41-91aa-4563-b2c8-083c6fc88ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 41642])\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "signal, sr = torchaudio.load('/scratch/network/mk8574/audio_sentiment_challenge/data/train/TRAIN_0001.wav')\n",
    "print(signal.shape)\n",
    "print(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36b966ec-624b-4cf6-a7c3-d05834458b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import augly.audio as audaugs\n",
    "import augly.utils as utils\n",
    "\n",
    "aug = audaugs.Compose([\n",
    "    audaugs.AddBackgroundNoise(p = 0.1),\n",
    "    audaugs.Clip(duration_factor = 0.7),\n",
    "#    audaugs.TimeStretch(rate = 3.0),\n",
    "#    audaugs.Speed(factor = 3.0),\n",
    "    audaugs.Harmonic(p = 0.5),\n",
    "    audaugs.InvertChannels(),\n",
    "    audaugs.OneOf([audaugs.Clicks(p = 0.6),\n",
    "                   audaugs.InsertInBackground(offset_factor = 0.25, p = 0.6)\n",
    "                   ])\n",
    "#    audaugs.ToMono()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "50d57e7a-d8e8-4179-b725-f65d2c9527e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from augly.audio.utils import validate_and_load_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2644c35e-575f-4925-9a3c-1fa575f08ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSentDataset(Dataset):\n",
    "    def __init__(self, df, transformation, target_sample_rate, duration, mode):\n",
    "        self.audio_paths = df['path'].values\n",
    "        if mode in [\"train\",\"valid\"]:\n",
    "            self.labels = df['label'].values\n",
    "        self.transformation = transformation # transformation\n",
    "        self.target_sample_rate = target_sample_rate # sample rate\n",
    "        self.num_samples = target_sample_rate * duration\n",
    "        self.mode = mode # ['train', 'valid', 'test']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_path = os.path.join('/scratch/network/mk8574/audio_sentiment_challenge/data', self.audio_paths[index])\n",
    "\n",
    "        #signal, sr = torchaudio.load(audio_path) # loaded the audio\n",
    "        signal, sr = validate_and_load_audio(audio_path)\n",
    "        \n",
    "        # Now we first checked if the sample rate is same as TARGET_SAMPLE_RATE and if it not equal we perform resampling\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        signal = torch.Tensor(signal)\n",
    "        # IN CASE DATA IS STEREO:\n",
    "        # Next we check the number of channels of the signal\n",
    "        #signal -> (num_channels, num_samples) - Eg.-(2, 14000) -> (1, 14000)\n",
    "        # if signal.shape[0]>1:\n",
    "        #     signalnu = torch.mean(signal, axis=0, keepdim=True)\n",
    "        \n",
    "        print(signal.shape)\n",
    "        # Lastly we check the number of samples of the signal\n",
    "        #signal -> (num_channels, num_samples) - Eg.-(1, 14000) -> (1, self.num_samples)\n",
    "        # If it is more than the required number of samples, we truncate the signal\n",
    "        if signal.shape[0] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        \n",
    "        # If it is less than the required number of samples, we pad the signal\n",
    "        if signal.shape[0]<self.num_samples:\n",
    "            num_missing_samples = self.num_samples - signal.shape[0]\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = F.pad(signal, last_dim_padding)\n",
    "        signal = np.array(signal)\n",
    "        signal, _ = aug(signal, sample_rate = sr, metadata = [])\n",
    "        # Finally all the process has been done and now we will extract mel spectrogram from the signal\n",
    "        mel = self.transformation(signal)\n",
    "        \n",
    "        # For pretrained models, we need 3 channel image, so for that we concatenate the extracted mel\n",
    "        image = torch.cat([mel, mel, mel])\n",
    "    \n",
    "        # Normalize the image\n",
    "        max_val = torch.abs(image).max()\n",
    "        \n",
    "        image = image / max_val\n",
    "        image=image[0]\n",
    "        \n",
    "        \n",
    "        if self.mode in ['train', 'valid']:\n",
    "            label = torch.tensor(self.labels[index])\n",
    "            return image, label\n",
    "        \n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f30696b-3abf-4af3-9a72-108ee10ca846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=config.sample_rate, \n",
    "                                                      n_fft=config.n_fft, \n",
    "                                                      hop_length=config.hop_length, \n",
    "                                                      n_mels=config.n_mels)\n",
    "# Function to get data according to the folds\n",
    "def get_data():\n",
    "    df = pd.read_csv('/scratch/network/mk8574/audio_sentiment_challenge/data/train.csv')\n",
    "    train_df, valid_df = train_test_split(df, test_size = 0.2, shuffle = True)\n",
    "    \n",
    "    train_dataset = AudioSentDataset(train_df, mel_spectrogram, config.sample_rate, config.duration, mode = 'train')\n",
    "    valid_dataset = AudioSentDataset(valid_df, mel_spectrogram, config.sample_rate, config.duration, mode = 'valid')\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=config.valid_batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e829ddd-2eb1-47d5-b84f-2616a9a97b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEFResnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BirdCLEFResnet, self).__init__()\n",
    "        self.base_model = models.__getattribute__(config.model_name)(pretrained=True)\n",
    "        \n",
    "        #self.base_model = torchaudio.models.hubert_pretrain_base(num_classes=6) \n",
    "        #for param in self.base_model.parameters():\n",
    "            #param.requires_grad = False\n",
    "            \n",
    "        #in_features = self.base_model.head.out_features\n",
    "        \n",
    "        #self.base_model.head.out_features = nn.Linear(1028, config.num_classes)\n",
    "\n",
    "    def forward(self, x,labels):\n",
    "        x = self.base_model(x,labels)\n",
    "        return x, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc3edb71-2741-4644-9440-f3e3a49c6e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BirdCLEFResnet(\n",
       "  (base_model): HuBERTPretrainModel(\n",
       "    (wav2vec2): Wav2Vec2Model(\n",
       "      (feature_extractor): FeatureExtractor(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): ConvLayerBlock(\n",
       "            (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          )\n",
       "          (1-4): 4 x ConvLayerBlock(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          )\n",
       "          (5-6): 2 x ConvLayerBlock(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (encoder): Encoder(\n",
       "        (feature_projection): FeatureProjection(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
       "            (conv): ParametrizedConv1d(\n",
       "              768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x EncoderLayer(\n",
       "              (attention): SelfAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (feed_forward): FeedForward(\n",
       "                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mask_generator): MaskGenerator()\n",
       "    (logit_generator): LogitGenerator(\n",
       "      (final_proj): Linear(in_features=768, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BirdCLEFResnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "85dc3074-6099-423b-a910-b8fe143f45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    SMOOTH = 1e-10\n",
    "    \n",
    "    return nn.CrossEntropyLoss(label_smoothing = 0.3)(outputs + SMOOTH, labels)\n",
    "\n",
    "def train(model, data_loader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    loop = tqdm(data_loader, position=0)\n",
    "    for i, (mels, labels) in enumerate(loop):\n",
    "        mels = mels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(mels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{config.epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    return running_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fac948-973b-46e9-bee2-ab939953ecea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f0d0379a-e5ad-4ad7-a7c1-c02b0e20a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, data_loader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    \n",
    "    loop = tqdm(data_loader, position=0)\n",
    "    for mels, labels in loop:\n",
    "        mels = mels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(mels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print('Outputs:', outputs)\n",
    "        # print('Labels:', labels)\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        pred.extend(preds.view(-1).cpu().detach().numpy())\n",
    "        label.extend(labels.view(-1).cpu().detach().numpy())\n",
    "        \n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{config.epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    valid_f1 = f1_score(label, pred, average='macro')\n",
    "    label = torch.Tensor(label)\n",
    "    pred = torch.Tensor(pred)\n",
    "    valid_acc = (label == pred).float().sum() / label.shape[0]\n",
    "    \n",
    "    return running_loss/len(data_loader), valid_f1, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "582ef36d-fcd6-4a19-a6da-1bce27c90524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1, 2, 3])\n",
    "b = torch.Tensor([1, 4, 5])\n",
    "\n",
    "print((a == b).float().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "443435f9-324c-4171-9205-2479ee004d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    train_loader, valid_loader = get_data()\n",
    "    \n",
    "    model = BirdCLEFResnet().to(config.device)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=10)\n",
    "    \n",
    "    best_valid_f1 = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, scheduler, config.device, epoch)\n",
    "        valid_loss, valid_f1, valid_acc = valid(model, valid_loader, config.device, epoch)\n",
    "        \n",
    "        print(f\"Validation F1 - {valid_f1}, Accuracy - {valid_acc}\")\n",
    "        torch.save(model.state_dict(), f'./model.bin')\n",
    "        print(f\"Saved model checkpoint at ./model.bin\")\n",
    "\n",
    "    return best_valid_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4722e5b2-feb3-4f6d-ac85-e286d3c233bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28829])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograd\n\u001b[1;32m      2\u001b[0m autograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 12\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m best_valid_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m---> 12\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     valid_loss, valid_f1, valid_acc \u001b[38;5;241m=\u001b[39m valid(model, valid_loader, config\u001b[38;5;241m.\u001b[39mdevice, epoch)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation F1 - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_f1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[71], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, scheduler, device, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(data_loader, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (mels, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loop):\n\u001b[1;32m     12\u001b[0m     mels \u001b[38;5;241m=\u001b[39m mels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[84], line 46\u001b[0m, in \u001b[0;36mAudioSentDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     44\u001b[0m signal, _ \u001b[38;5;241m=\u001b[39m aug(signal, sample_rate \u001b[38;5;241m=\u001b[39m sr, metadata \u001b[38;5;241m=\u001b[39m [])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Finally all the process has been done and now we will extract mel spectrogram from the signal\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# For pretrained models, we need 3 channel image, so for that we concatenate the extracted mel\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#image = torch.cat([mel, mel, mel])\u001b[39;00m\n\u001b[1;32m     50\u001b[0m image \u001b[38;5;241m=\u001b[39m mel\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:619\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    612\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;124;03m        Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 619\u001b[0m     specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_scale(specgram)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:110\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torchaudio/functional/functional.py:122\u001b[0m, in \u001b[0;36mspectrogram\u001b[0;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[1;32m    119\u001b[0m frame_length_norm, window_norm \u001b[38;5;241m=\u001b[39m _get_spec_norms(normalized)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# pack batch\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[43mwaveform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "from torch import autograd\n",
    "autograd.set_detect_anomaly(True)\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b54b22-ebe0-4e50-914a-3a2c80614acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = torchaudio.transforms.MFCC(sample_rate = config.sample_rate,\n",
    "                                 n_mfcc = 20,\n",
    "                                 log_mels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88218a3-5d72-4d58-a124-39575cd8832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    PATH = \"/scratch/network/mk8574/audio_sentiment_challenge/baseline_dy/model_20231122015145.bin\"\n",
    "    \n",
    "    test_df = pd.read_csv('../data/test.csv')\n",
    "    \n",
    "    \n",
    "    model = BirdCLEFResnet().to(config.device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    test_dataset = AudioSentDataset(test_df, mfcc, config.sample_rate, config.duration, mode = 'test')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32,shuffle=False)\n",
    "    \n",
    "    \n",
    "    test_df = test_df.drop(['path'], axis = 1)\n",
    "    ans = []\n",
    "    for i, mels in enumerate(tqdm(test_loader, position=0)):   \n",
    "        mels = mels.to(config.device)\n",
    "\n",
    "        mels = torch.argmax(model(mels), dim = 1)\n",
    "        \n",
    "        ans.append(mels)\n",
    "    print(ans)\n",
    "    z = [y.item() for x in ans for y in x]\n",
    "\n",
    "    test_df['label'] = z\n",
    "    test_df.to_csv('submission.csv', index = False)\n",
    "    \n",
    "    print(test_df)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c38d76-c621-4de5-a2f0-1a9f126b124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e125d-d803-40ef-b134-c45da7e685fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44183b3f-e49e-474b-935d-d071b145aa7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6bf3f-5f54-4a50-a5ef-7ebc915f881c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb482e3-deac-4b9f-aac6-79a261c8987d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd33ae7b-3003-45fd-8618-7f30b81fb9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk8574_3.10 [~/.conda/envs/mk8574_3.10/]",
   "language": "python",
   "name": "conda_mk8574_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
