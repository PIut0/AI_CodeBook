{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3645962a-4d23-486d-8b84-24435ad0b31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at Rajaram1996/Hubert_emotion and are newly initialized: ['classifier.weight', 'classifier.bias', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'projector.weight', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 5001/5001 [00:20<00:00, 241.96it/s]\n",
      "100%|██████████| 1881/1881 [00:09<00:00, 202.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"audio-classification\", model=\"Rajaram1996/Hubert_emotion\")\n",
    "\n",
    "\n",
    "CFG = {\n",
    "    'SR':16000,\n",
    "    'N_MFCC':32, # Melspectrogram 벡터를 추출할 개수\n",
    "    'SEED':42\n",
    "}\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정\n",
    "\n",
    "working_dir = '/scratch/network/mk8574/audio_sentiment_challenge'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(working_dir, 'data', 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(working_dir, 'data', 'test.csv'))\n",
    "\n",
    "def get_mfcc_feature(df):\n",
    "    features = []\n",
    "    for path in tqdm(df['path']):\n",
    "        # librosa패키지를 사용하여 wav 파일 load\n",
    "        wav_path = os.path.join(working_dir, 'data', path)\n",
    "        y, sr = librosa.load(wav_path, sr=CFG['SR'])\n",
    "        # librosa패키지를 사용하여 mfcc 추출\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CFG['N_MFCC'])\n",
    "        y_feature = []\n",
    "        # 추출된 MFCC들의 평균을 Feature로 사용\n",
    "        for e in mfcc:\n",
    "            y_feature.append(np.mean(e))\n",
    "        features.append(y_feature)\n",
    "\n",
    "    mfcc_df = pd.DataFrame(features, columns=['mfcc_'+str(x) for x in range(1,CFG['N_MFCC']+1)])\n",
    "    return mfcc_df\n",
    "\n",
    "train_x = get_mfcc_feature(train_df)\n",
    "test_x = get_mfcc_feature(test_df)\n",
    "\n",
    "train_y = train_df['label']\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=CFG['SEED'])\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "preds = model.predict(test_x)\n",
    "\n",
    "# submission = pd.read_csv(os.path.join(working_dir, 'baseline_dy', 'sample_submission.csv'))\n",
    "# submission['label'] = preds\n",
    "# submission.to_csv(os.path.join(working_dir, 'baseline_dy', 'baseline_submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0b900-8803-49bb-9a1b-b8e446f567e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be3f68-d09e-4c10-998f-cf6f56f3e309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff10104-1215-4215-b053-fb2c617a2035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de62a77b-08aa-4b08-a0f2-fb5e0392cb32",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dataset() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m train_img_paths, val_img_paths \u001b[38;5;241m=\u001b[39m train_test_split(train_img_paths, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEED\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m test_img_paths \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_dirs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.wav\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 18\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_img_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset(paths \u001b[38;5;241m=\u001b[39m test_img_paths)\n\u001b[1;32m     20\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     21\u001b[0m     train_dataset,\n\u001b[1;32m     22\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39mpin_memory,\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Dataset() takes no arguments"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "batch_size = 64\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 4\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "train_df = pd.read_csv(os.path.join(working_dir, 'data', 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(working_dir, 'data', 'test.csv'))\n",
    "\n",
    "train_dirs = \"/scratch/network/mk8574/audio_sentiment_challenge/data\"\n",
    "train_img_paths = glob(os.path.join(train_dirs, 'train', '*.wav'))\n",
    "train_img_paths, val_img_paths = train_test_split(train_img_paths, test_size=0.2, random_state=CFG['SEED'], shuffle=True)\n",
    "test_img_paths = glob(os.path.join(train_dirs, 'test', '*.wav'))\n",
    "train_dataset = torch.utils.data.Dataset(paths=train_img_paths)\n",
    "test_dataset = torch.utils.data.Dataset(paths = test_img_paths)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478d48ff-6346-43f7-9071-7d6cfcf22767",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m model \u001b[38;5;241m=\u001b[39m M5(n_input\u001b[38;5;241m=\u001b[39m\u001b[43mtransformed\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(labels))\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformed' is not defined"
     ]
    }
   ],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "\n",
    "model = M5(n_input=transformed.shape[0], n_output=len(labels))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb3207a-fbe3-4a72-b9de-2c5edf6b3dc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m      2\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0c3b53-412b-4763-ba6e-2291c2f4644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be970323-3dd9-44e9-b79f-da479497f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c723a58-b411-4be6-921c-d9f477ea8ed4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m log_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      2\u001b[0m n_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 4\u001b[0m pbar_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_loader\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_loader))\n\u001b[1;32m      5\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# The transform needs to live on the same device as the model and the data.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "log_interval = 20\n",
    "n_epoch = 2\n",
    "\n",
    "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "losses = []\n",
    "\n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "transform = transform.to(device)\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        train(model, epoch, log_interval)\n",
    "        test(model, epoch)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239f7d57-5394-42a2-8a21-3579dfa5cb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__name__',\n",
       " '__doc__',\n",
       " '__package__',\n",
       " '__loader__',\n",
       " '__spec__',\n",
       " '_modules',\n",
       " '_class_to_module',\n",
       " '__all__',\n",
       " '__file__',\n",
       " '__path__',\n",
       " '_objects',\n",
       " '_name',\n",
       " '_import_structure',\n",
       " 'file_utils',\n",
       " 'convert_slow_tokenizer',\n",
       " 'dynamic_module_utils',\n",
       " 'tokenization_utils_base',\n",
       " 'tokenization_utils',\n",
       " 'tokenization_utils_fast',\n",
       " 'models',\n",
       " 'audio_utils',\n",
       " 'feature_extraction_utils',\n",
       " 'feature_extraction_sequence_utils',\n",
       " 'WhisperFeatureExtractor',\n",
       " 'benchmark',\n",
       " 'commands',\n",
       " 'configuration_utils',\n",
       " 'convert_graph_to_onnx',\n",
       " 'convert_slow_tokenizers_checkpoints_to_fast',\n",
       " 'convert_tf_hub_seq_to_seq_bert_to_pytorch',\n",
       " 'data',\n",
       " 'data.data_collator',\n",
       " 'data.metrics',\n",
       " 'data.processors',\n",
       " 'debug_utils',\n",
       " 'deepspeed',\n",
       " 'dependency_versions_check',\n",
       " 'dependency_versions_table',\n",
       " 'generation',\n",
       " 'hf_argparser',\n",
       " 'hyperparameter_search',\n",
       " 'image_transforms',\n",
       " 'integrations',\n",
       " 'modelcard',\n",
       " 'modeling_tf_pytorch_utils',\n",
       " 'models.albert',\n",
       " 'models.align',\n",
       " 'models.altclip',\n",
       " 'models.audio_spectrogram_transformer',\n",
       " 'models.auto',\n",
       " 'models.autoformer',\n",
       " 'models.bark',\n",
       " 'models.bart',\n",
       " 'models.barthez',\n",
       " 'models.bartpho',\n",
       " 'models.beit',\n",
       " 'models.bert',\n",
       " 'models.bert_generation',\n",
       " 'models.bert_japanese',\n",
       " 'models.bertweet',\n",
       " 'models.big_bird',\n",
       " 'models.bigbird_pegasus',\n",
       " 'models.biogpt',\n",
       " 'models.bit',\n",
       " 'models.blenderbot',\n",
       " 'models.blenderbot_small',\n",
       " 'models.blip',\n",
       " 'models.blip_2',\n",
       " 'models.bloom',\n",
       " 'models.bridgetower',\n",
       " 'models.bros',\n",
       " 'models.byt5',\n",
       " 'models.camembert',\n",
       " 'models.canine',\n",
       " 'models.chinese_clip',\n",
       " 'models.clap',\n",
       " 'models.clip',\n",
       " 'models.clipseg',\n",
       " 'models.clvp',\n",
       " 'models.code_llama',\n",
       " 'models.codegen',\n",
       " 'models.conditional_detr',\n",
       " 'models.convbert',\n",
       " 'models.convnext',\n",
       " 'models.convnextv2',\n",
       " 'models.cpm',\n",
       " 'models.cpmant',\n",
       " 'models.ctrl',\n",
       " 'models.cvt',\n",
       " 'models.data2vec',\n",
       " 'models.deberta',\n",
       " 'models.deberta_v2',\n",
       " 'models.decision_transformer',\n",
       " 'models.deformable_detr',\n",
       " 'models.deit',\n",
       " 'models.deprecated',\n",
       " 'models.deprecated.bort',\n",
       " 'models.deprecated.mctct',\n",
       " 'models.deprecated.mmbt',\n",
       " 'models.deprecated.open_llama',\n",
       " 'models.deprecated.retribert',\n",
       " 'models.deprecated.tapex',\n",
       " 'models.deprecated.trajectory_transformer',\n",
       " 'models.deprecated.van',\n",
       " 'models.deta',\n",
       " 'models.detr',\n",
       " 'models.dialogpt',\n",
       " 'models.dinat',\n",
       " 'models.dinov2',\n",
       " 'models.distilbert',\n",
       " 'models.dit',\n",
       " 'models.donut',\n",
       " 'models.dpr',\n",
       " 'models.dpt',\n",
       " 'models.efficientformer',\n",
       " 'models.efficientnet',\n",
       " 'models.electra',\n",
       " 'models.encodec',\n",
       " 'models.encoder_decoder',\n",
       " 'models.ernie',\n",
       " 'models.ernie_m',\n",
       " 'models.esm',\n",
       " 'models.falcon',\n",
       " 'models.flaubert',\n",
       " 'models.flava',\n",
       " 'models.fnet',\n",
       " 'models.focalnet',\n",
       " 'models.fsmt',\n",
       " 'models.funnel',\n",
       " 'models.fuyu',\n",
       " 'models.git',\n",
       " 'models.glpn',\n",
       " 'models.gpt2',\n",
       " 'models.gpt_bigcode',\n",
       " 'models.gpt_neo',\n",
       " 'models.gpt_neox',\n",
       " 'models.gpt_neox_japanese',\n",
       " 'models.gpt_sw3',\n",
       " 'models.gptj',\n",
       " 'models.gptsan_japanese',\n",
       " 'models.graphormer',\n",
       " 'models.groupvit',\n",
       " 'models.herbert',\n",
       " 'models.hubert',\n",
       " 'models.ibert',\n",
       " 'models.idefics',\n",
       " 'models.imagegpt',\n",
       " 'models.informer',\n",
       " 'models.instructblip',\n",
       " 'models.jukebox',\n",
       " 'models.kosmos2',\n",
       " 'models.layoutlm',\n",
       " 'models.layoutlmv2',\n",
       " 'models.layoutlmv3',\n",
       " 'models.layoutxlm',\n",
       " 'models.led',\n",
       " 'models.levit',\n",
       " 'models.lilt',\n",
       " 'models.llama',\n",
       " 'models.longformer',\n",
       " 'models.longt5',\n",
       " 'models.luke',\n",
       " 'models.lxmert',\n",
       " 'models.m2m_100',\n",
       " 'models.marian',\n",
       " 'models.markuplm',\n",
       " 'models.mask2former',\n",
       " 'models.maskformer',\n",
       " 'models.mbart',\n",
       " 'models.mbart50',\n",
       " 'models.mega',\n",
       " 'models.megatron_bert',\n",
       " 'models.megatron_gpt2',\n",
       " 'models.mgp_str',\n",
       " 'models.mistral',\n",
       " 'models.mluke',\n",
       " 'models.mobilebert',\n",
       " 'models.mobilenet_v1',\n",
       " 'models.mobilenet_v2',\n",
       " 'models.mobilevit',\n",
       " 'models.mobilevitv2',\n",
       " 'models.mpnet',\n",
       " 'models.mpt',\n",
       " 'models.mra',\n",
       " 'models.mt5',\n",
       " 'models.musicgen',\n",
       " 'models.mvp',\n",
       " 'models.nat',\n",
       " 'models.nezha',\n",
       " 'models.nllb',\n",
       " 'models.nllb_moe',\n",
       " 'models.nougat',\n",
       " 'models.nystromformer',\n",
       " 'models.oneformer',\n",
       " 'models.openai',\n",
       " 'models.opt',\n",
       " 'models.owlv2',\n",
       " 'models.owlvit',\n",
       " 'models.pegasus',\n",
       " 'models.pegasus_x',\n",
       " 'models.perceiver',\n",
       " 'models.persimmon',\n",
       " 'models.phi',\n",
       " 'models.phobert',\n",
       " 'models.pix2struct',\n",
       " 'models.plbart',\n",
       " 'models.poolformer',\n",
       " 'models.pop2piano',\n",
       " 'models.prophetnet',\n",
       " 'models.pvt',\n",
       " 'models.qdqbert',\n",
       " 'models.rag',\n",
       " 'models.realm',\n",
       " 'models.reformer',\n",
       " 'models.regnet',\n",
       " 'models.rembert',\n",
       " 'models.resnet',\n",
       " 'models.roberta',\n",
       " 'models.roberta_prelayernorm',\n",
       " 'models.roc_bert',\n",
       " 'models.roformer',\n",
       " 'models.rwkv',\n",
       " 'models.sam',\n",
       " 'models.seamless_m4t',\n",
       " 'models.segformer',\n",
       " 'models.sew',\n",
       " 'models.sew_d',\n",
       " 'models.speech_encoder_decoder',\n",
       " 'models.speech_to_text',\n",
       " 'models.speech_to_text_2',\n",
       " 'models.speecht5',\n",
       " 'models.splinter',\n",
       " 'models.squeezebert',\n",
       " 'models.swiftformer',\n",
       " 'models.swin',\n",
       " 'models.swin2sr',\n",
       " 'models.swinv2',\n",
       " 'models.switch_transformers',\n",
       " 'models.t5',\n",
       " 'models.table_transformer',\n",
       " 'models.tapas',\n",
       " 'models.time_series_transformer',\n",
       " 'models.timesformer',\n",
       " 'models.timm_backbone',\n",
       " 'models.transfo_xl',\n",
       " 'models.trocr',\n",
       " 'models.tvlt',\n",
       " 'models.umt5',\n",
       " 'models.unispeech',\n",
       " 'models.unispeech_sat',\n",
       " 'models.upernet',\n",
       " 'models.videomae',\n",
       " 'models.vilt',\n",
       " 'models.vision_encoder_decoder',\n",
       " 'models.vision_text_dual_encoder',\n",
       " 'models.visual_bert',\n",
       " 'models.vit',\n",
       " 'models.vit_hybrid',\n",
       " 'models.vit_mae',\n",
       " 'models.vit_msn',\n",
       " 'models.vitdet',\n",
       " 'models.vitmatte',\n",
       " 'models.vits',\n",
       " 'models.vivit',\n",
       " 'models.wav2vec2',\n",
       " 'models.wav2vec2_conformer',\n",
       " 'models.wav2vec2_phoneme',\n",
       " 'models.wav2vec2_with_lm',\n",
       " 'models.wavlm',\n",
       " 'models.whisper',\n",
       " 'models.x_clip',\n",
       " 'models.xglm',\n",
       " 'models.xlm',\n",
       " 'models.xlm_prophetnet',\n",
       " 'models.xlm_roberta',\n",
       " 'models.xlm_roberta_xl',\n",
       " 'models.xlnet',\n",
       " 'models.xmod',\n",
       " 'models.yolos',\n",
       " 'models.yoso',\n",
       " 'onnx',\n",
       " 'pipelines',\n",
       " 'processing_utils',\n",
       " 'testing_utils',\n",
       " 'tools',\n",
       " 'trainer_callback',\n",
       " 'trainer_utils',\n",
       " 'training_args',\n",
       " 'training_args_seq2seq',\n",
       " 'training_args_tf',\n",
       " 'utils',\n",
       " 'utils.quantization_config',\n",
       " 'utils.dummy_sentencepiece_objects',\n",
       " 'utils.dummy_sentencepiece_and_tokenizers_objects',\n",
       " 'utils.dummy_tensorflow_text_objects',\n",
       " 'utils.dummy_keras_nlp_objects',\n",
       " 'image_processing_utils',\n",
       " 'image_utils',\n",
       " 'activations',\n",
       " 'benchmark.benchmark',\n",
       " 'benchmark.benchmark_args',\n",
       " 'data.datasets',\n",
       " 'generation_utils',\n",
       " 'modeling_outputs',\n",
       " 'modeling_utils',\n",
       " 'optimization',\n",
       " 'pytorch_utils',\n",
       " 'sagemaker',\n",
       " 'time_series_utils',\n",
       " 'trainer',\n",
       " 'trainer_pt_utils',\n",
       " 'trainer_seq2seq',\n",
       " 'utils.dummy_tf_objects',\n",
       " 'utils.dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects',\n",
       " 'utils.dummy_flax_objects',\n",
       " 'PretrainedConfig',\n",
       " 'DataProcessor',\n",
       " 'InputExample',\n",
       " 'InputFeatures',\n",
       " 'SingleSentenceClassificationProcessor',\n",
       " 'SquadExample',\n",
       " 'SquadFeatures',\n",
       " 'SquadV1Processor',\n",
       " 'SquadV2Processor',\n",
       " 'glue_compute_metrics',\n",
       " 'glue_convert_examples_to_features',\n",
       " 'glue_output_modes',\n",
       " 'glue_processors',\n",
       " 'glue_tasks_num_labels',\n",
       " 'squad_convert_examples_to_features',\n",
       " 'xnli_compute_metrics',\n",
       " 'xnli_output_modes',\n",
       " 'xnli_processors',\n",
       " 'xnli_tasks_num_labels',\n",
       " 'DataCollator',\n",
       " 'DataCollatorForLanguageModeling',\n",
       " 'DataCollatorForPermutationLanguageModeling',\n",
       " 'DataCollatorForSeq2Seq',\n",
       " 'DataCollatorForSOP',\n",
       " 'DataCollatorForTokenClassification',\n",
       " 'DataCollatorForWholeWordMask',\n",
       " 'DataCollatorWithPadding',\n",
       " 'DefaultDataCollator',\n",
       " 'default_data_collator',\n",
       " 'SequenceFeatureExtractor',\n",
       " 'BatchFeature',\n",
       " 'FeatureExtractionMixin',\n",
       " 'GenerationConfig',\n",
       " 'TextIteratorStreamer',\n",
       " 'TextStreamer',\n",
       " 'AlternatingCodebooksLogitsProcessor',\n",
       " 'BeamScorer',\n",
       " 'BeamSearchScorer',\n",
       " 'ClassifierFreeGuidanceLogitsProcessor',\n",
       " 'ConstrainedBeamSearchScorer',\n",
       " 'Constraint',\n",
       " 'ConstraintListState',\n",
       " 'DisjunctiveConstraint',\n",
       " 'EncoderNoRepeatNGramLogitsProcessor',\n",
       " 'EncoderRepetitionPenaltyLogitsProcessor',\n",
       " 'EpsilonLogitsWarper',\n",
       " 'EtaLogitsWarper',\n",
       " 'ExponentialDecayLengthPenalty',\n",
       " 'ForcedBOSTokenLogitsProcessor',\n",
       " 'ForcedEOSTokenLogitsProcessor',\n",
       " 'ForceTokensLogitsProcessor',\n",
       " 'GenerationMixin',\n",
       " 'HammingDiversityLogitsProcessor',\n",
       " 'InfNanRemoveLogitsProcessor',\n",
       " 'LogitNormalization',\n",
       " 'LogitsProcessor',\n",
       " 'LogitsProcessorList',\n",
       " 'LogitsWarper',\n",
       " 'MaxLengthCriteria',\n",
       " 'MaxTimeCriteria',\n",
       " 'MinLengthLogitsProcessor',\n",
       " 'MinNewTokensLengthLogitsProcessor',\n",
       " 'NoBadWordsLogitsProcessor',\n",
       " 'NoRepeatNGramLogitsProcessor',\n",
       " 'PhrasalConstraint',\n",
       " 'PrefixConstrainedLogitsProcessor',\n",
       " 'RepetitionPenaltyLogitsProcessor',\n",
       " 'SequenceBiasLogitsProcessor',\n",
       " 'StoppingCriteria',\n",
       " 'StoppingCriteriaList',\n",
       " 'SuppressTokensAtBeginLogitsProcessor',\n",
       " 'SuppressTokensLogitsProcessor',\n",
       " 'TemperatureLogitsWarper',\n",
       " 'TopKLogitsWarper',\n",
       " 'TopPLogitsWarper',\n",
       " 'TypicalLogitsWarper',\n",
       " 'UnbatchedClassifierFreeGuidanceLogitsProcessor',\n",
       " 'WhisperTimeStampLogitsProcessor',\n",
       " 'top_k_top_p_filtering',\n",
       " 'HfArgumentParser',\n",
       " 'is_clearml_available',\n",
       " 'is_comet_available',\n",
       " 'is_dvclive_available',\n",
       " 'is_neptune_available',\n",
       " 'is_optuna_available',\n",
       " 'is_ray_available',\n",
       " 'is_ray_tune_available',\n",
       " 'is_sigopt_available',\n",
       " 'is_tensorboard_available',\n",
       " 'is_wandb_available',\n",
       " 'ModelCard',\n",
       " 'convert_tf_weight_name_to_pt_weight_name',\n",
       " 'load_pytorch_checkpoint_in_tf2_model',\n",
       " 'load_pytorch_model_in_tf2_model',\n",
       " 'load_pytorch_weights_in_tf2_model',\n",
       " 'load_tf2_checkpoint_in_pytorch_model',\n",
       " 'load_tf2_model_in_pytorch_model',\n",
       " 'load_tf2_weights_in_pytorch_model',\n",
       " 'ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'AlbertConfig',\n",
       " 'AlbertTokenizerFast',\n",
       " 'ALBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'AlbertForMaskedLM',\n",
       " 'AlbertForMultipleChoice',\n",
       " 'AlbertForPreTraining',\n",
       " 'AlbertForQuestionAnswering',\n",
       " 'AlbertForSequenceClassification',\n",
       " 'AlbertForTokenClassification',\n",
       " 'AlbertModel',\n",
       " 'AlbertPreTrainedModel',\n",
       " 'load_tf_weights_in_albert',\n",
       " 'ALIGN_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'AlignConfig',\n",
       " 'AlignProcessor',\n",
       " 'AlignTextConfig',\n",
       " 'AlignVisionConfig',\n",
       " 'ALIGN_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'AlignModel',\n",
       " 'AlignPreTrainedModel',\n",
       " 'AlignTextModel',\n",
       " 'AlignVisionModel',\n",
       " 'ALTCLIP_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'AltCLIPConfig',\n",
       " 'AltCLIPProcessor',\n",
       " 'AltCLIPTextConfig',\n",
       " 'AltCLIPVisionConfig',\n",
       " 'ALTCLIP_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'AltCLIPModel',\n",
       " 'AltCLIPPreTrainedModel',\n",
       " 'AltCLIPTextModel',\n",
       " 'AltCLIPVisionModel',\n",
       " 'AUDIO_SPECTROGRAM_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ASTConfig',\n",
       " 'ASTFeatureExtractor',\n",
       " 'AUDIO_SPECTROGRAM_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ASTForAudioClassification',\n",
       " 'ASTModel',\n",
       " 'ASTPreTrainedModel',\n",
       " 'ALL_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CONFIG_MAPPING',\n",
       " 'FEATURE_EXTRACTOR_MAPPING',\n",
       " 'IMAGE_PROCESSOR_MAPPING',\n",
       " 'MODEL_NAMES_MAPPING',\n",
       " 'PROCESSOR_MAPPING',\n",
       " 'TOKENIZER_MAPPING',\n",
       " 'AutoConfig',\n",
       " 'AutoFeatureExtractor',\n",
       " 'AutoImageProcessor',\n",
       " 'AutoProcessor',\n",
       " 'AutoTokenizer',\n",
       " 'MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING',\n",
       " 'MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING',\n",
       " 'MODEL_FOR_AUDIO_XVECTOR_MAPPING',\n",
       " 'MODEL_FOR_BACKBONE_MAPPING',\n",
       " 'MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING',\n",
       " 'MODEL_FOR_CAUSAL_LM_MAPPING',\n",
       " 'MODEL_FOR_CTC_MAPPING',\n",
       " 'MODEL_FOR_DEPTH_ESTIMATION_MAPPING',\n",
       " 'MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING',\n",
       " 'MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING',\n",
       " 'MODEL_FOR_IMAGE_SEGMENTATION_MAPPING',\n",
       " 'MODEL_FOR_IMAGE_TO_IMAGE_MAPPING',\n",
       " 'MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING',\n",
       " 'MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING',\n",
       " 'MODEL_FOR_MASKED_LM_MAPPING',\n",
       " 'MODEL_FOR_MASK_GENERATION_MAPPING',\n",
       " 'MODEL_FOR_MULTIPLE_CHOICE_MAPPING',\n",
       " 'MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING',\n",
       " 'MODEL_FOR_OBJECT_DETECTION_MAPPING',\n",
       " 'MODEL_FOR_PRETRAINING_MAPPING',\n",
       " 'MODEL_FOR_QUESTION_ANSWERING_MAPPING',\n",
       " 'MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING',\n",
       " 'MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING',\n",
       " 'MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING',\n",
       " 'MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING',\n",
       " 'MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING',\n",
       " 'MODEL_FOR_TEXT_ENCODING_MAPPING',\n",
       " 'MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING',\n",
       " 'MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING',\n",
       " 'MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING',\n",
       " 'MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING',\n",
       " 'MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING',\n",
       " 'MODEL_FOR_VISION_2_SEQ_MAPPING',\n",
       " 'MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING',\n",
       " 'MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING',\n",
       " 'MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING',\n",
       " 'MODEL_MAPPING',\n",
       " 'MODEL_WITH_LM_HEAD_MAPPING',\n",
       " 'AutoBackbone',\n",
       " 'AutoModel',\n",
       " 'AutoModelForAudioClassification',\n",
       " 'AutoModelForAudioFrameClassification',\n",
       " 'AutoModelForAudioXVector',\n",
       " 'AutoModelForCausalLM',\n",
       " 'AutoModelForCTC',\n",
       " 'AutoModelForDepthEstimation',\n",
       " 'AutoModelForDocumentQuestionAnswering',\n",
       " 'AutoModelForImageClassification',\n",
       " 'AutoModelForImageSegmentation',\n",
       " 'AutoModelForImageToImage',\n",
       " 'AutoModelForInstanceSegmentation',\n",
       " 'AutoModelForMaskedImageModeling',\n",
       " 'AutoModelForMaskedLM',\n",
       " 'AutoModelForMaskGeneration',\n",
       " 'AutoModelForMultipleChoice',\n",
       " 'AutoModelForNextSentencePrediction',\n",
       " 'AutoModelForObjectDetection',\n",
       " 'AutoModelForPreTraining',\n",
       " 'AutoModelForQuestionAnswering',\n",
       " 'AutoModelForSemanticSegmentation',\n",
       " 'AutoModelForSeq2SeqLM',\n",
       " 'AutoModelForSequenceClassification',\n",
       " 'AutoModelForSpeechSeq2Seq',\n",
       " 'AutoModelForTableQuestionAnswering',\n",
       " 'AutoModelForTextEncoding',\n",
       " 'AutoModelForTextToSpectrogram',\n",
       " 'AutoModelForTextToWaveform',\n",
       " 'AutoModelForTokenClassification',\n",
       " 'AutoModelForUniversalSegmentation',\n",
       " 'AutoModelForVideoClassification',\n",
       " 'AutoModelForVision2Seq',\n",
       " 'AutoModelForVisualQuestionAnswering',\n",
       " 'AutoModelForZeroShotImageClassification',\n",
       " 'AutoModelForZeroShotObjectDetection',\n",
       " 'AutoModelWithLMHead',\n",
       " 'AUTOFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'AutoformerConfig',\n",
       " 'AUTOFORMER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'AutoformerForPrediction',\n",
       " 'AutoformerModel',\n",
       " 'AutoformerPreTrainedModel',\n",
       " 'BarkCoarseConfig',\n",
       " 'BarkConfig',\n",
       " 'BarkFineConfig',\n",
       " 'BarkProcessor',\n",
       " 'BarkSemanticConfig',\n",
       " 'BARK_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BarkCausalModel',\n",
       " 'BarkCoarseModel',\n",
       " 'BarkFineModel',\n",
       " 'BarkModel',\n",
       " 'BarkPreTrainedModel',\n",
       " 'BarkSemanticModel',\n",
       " 'BartConfig',\n",
       " 'BartTokenizer',\n",
       " 'BartTokenizerFast',\n",
       " 'BART_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BartForCausalLM',\n",
       " 'BartForConditionalGeneration',\n",
       " 'BartForQuestionAnswering',\n",
       " 'BartForSequenceClassification',\n",
       " 'BartModel',\n",
       " 'BartPretrainedModel',\n",
       " 'BartPreTrainedModel',\n",
       " 'PretrainedBartModel',\n",
       " 'BarthezTokenizerFast',\n",
       " 'BEIT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BeitConfig',\n",
       " 'BeitFeatureExtractor',\n",
       " 'BeitImageProcessor',\n",
       " 'BEIT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BeitForImageClassification',\n",
       " 'BeitForMaskedImageModeling',\n",
       " 'BeitForSemanticSegmentation',\n",
       " 'BeitModel',\n",
       " 'BeitPreTrainedModel',\n",
       " 'BERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BasicTokenizer',\n",
       " 'BertConfig',\n",
       " 'BertTokenizer',\n",
       " 'WordpieceTokenizer',\n",
       " 'BertTokenizerFast',\n",
       " 'BERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BertForMaskedLM',\n",
       " 'BertForMultipleChoice',\n",
       " 'BertForNextSentencePrediction',\n",
       " 'BertForPreTraining',\n",
       " 'BertForQuestionAnswering',\n",
       " 'BertForSequenceClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BertLayer',\n",
       " 'BertLMHeadModel',\n",
       " 'BertModel',\n",
       " 'BertPreTrainedModel',\n",
       " 'load_tf_weights_in_bert',\n",
       " 'BertGenerationConfig',\n",
       " 'BertGenerationDecoder',\n",
       " 'BertGenerationEncoder',\n",
       " 'BertGenerationPreTrainedModel',\n",
       " 'load_tf_weights_in_bert_generation',\n",
       " 'BertJapaneseTokenizer',\n",
       " 'CharacterTokenizer',\n",
       " 'MecabTokenizer',\n",
       " 'BertweetTokenizer',\n",
       " 'BIG_BIRD_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BigBirdConfig',\n",
       " 'BigBirdTokenizerFast',\n",
       " 'BIG_BIRD_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BigBirdForCausalLM',\n",
       " 'BigBirdForMaskedLM',\n",
       " 'BigBirdForMultipleChoice',\n",
       " 'BigBirdForPreTraining',\n",
       " 'BigBirdForQuestionAnswering',\n",
       " 'BigBirdForSequenceClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'BigBirdLayer',\n",
       " 'BigBirdModel',\n",
       " 'BigBirdPreTrainedModel',\n",
       " 'load_tf_weights_in_big_bird',\n",
       " 'BIGBIRD_PEGASUS_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BigBirdPegasusConfig',\n",
       " 'BIGBIRD_PEGASUS_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BigBirdPegasusForCausalLM',\n",
       " 'BigBirdPegasusForConditionalGeneration',\n",
       " 'BigBirdPegasusForQuestionAnswering',\n",
       " 'BigBirdPegasusForSequenceClassification',\n",
       " 'BigBirdPegasusModel',\n",
       " 'BigBirdPegasusPreTrainedModel',\n",
       " 'BIOGPT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BioGptConfig',\n",
       " 'BioGptTokenizer',\n",
       " 'BIOGPT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BioGptForCausalLM',\n",
       " 'BioGptForSequenceClassification',\n",
       " 'BioGptForTokenClassification',\n",
       " 'BioGptModel',\n",
       " 'BioGptPreTrainedModel',\n",
       " 'BIT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BitConfig',\n",
       " 'BitImageProcessor',\n",
       " 'BIT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BitBackbone',\n",
       " 'BitForImageClassification',\n",
       " 'BitModel',\n",
       " 'BitPreTrainedModel',\n",
       " 'BLENDERBOT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BlenderbotConfig',\n",
       " 'BlenderbotTokenizer',\n",
       " 'BlenderbotTokenizerFast',\n",
       " 'BLENDERBOT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BlenderbotForCausalLM',\n",
       " 'BlenderbotForConditionalGeneration',\n",
       " 'BlenderbotModel',\n",
       " 'BlenderbotPreTrainedModel',\n",
       " 'BLENDERBOT_SMALL_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BlenderbotSmallConfig',\n",
       " 'BlenderbotSmallTokenizer',\n",
       " 'BlenderbotSmallTokenizerFast',\n",
       " 'BLENDERBOT_SMALL_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BlenderbotSmallForCausalLM',\n",
       " 'BlenderbotSmallForConditionalGeneration',\n",
       " 'BlenderbotSmallModel',\n",
       " 'BlenderbotSmallPreTrainedModel',\n",
       " 'BLIP_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BlipConfig',\n",
       " 'BlipProcessor',\n",
       " 'BlipTextConfig',\n",
       " 'BlipVisionConfig',\n",
       " 'BlipImageProcessor',\n",
       " 'BLIP_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BlipForConditionalGeneration',\n",
       " 'BlipForImageTextRetrieval',\n",
       " 'BlipForQuestionAnswering',\n",
       " 'BlipModel',\n",
       " 'BlipPreTrainedModel',\n",
       " 'BlipTextModel',\n",
       " 'BlipVisionModel',\n",
       " 'BLIP_2_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'Blip2Config',\n",
       " 'Blip2Processor',\n",
       " 'Blip2QFormerConfig',\n",
       " 'Blip2VisionConfig',\n",
       " 'BLIP_2_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'Blip2ForConditionalGeneration',\n",
       " 'Blip2Model',\n",
       " 'Blip2PreTrainedModel',\n",
       " 'Blip2QFormerModel',\n",
       " 'Blip2VisionModel',\n",
       " 'BLOOM_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BloomConfig',\n",
       " 'BloomTokenizerFast',\n",
       " 'BLOOM_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BloomForCausalLM',\n",
       " 'BloomForQuestionAnswering',\n",
       " 'BloomForSequenceClassification',\n",
       " 'BloomForTokenClassification',\n",
       " 'BloomModel',\n",
       " 'BloomPreTrainedModel',\n",
       " 'BRIDGETOWER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BridgeTowerConfig',\n",
       " 'BridgeTowerProcessor',\n",
       " 'BridgeTowerTextConfig',\n",
       " 'BridgeTowerVisionConfig',\n",
       " 'BridgeTowerImageProcessor',\n",
       " 'BRIDGETOWER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BridgeTowerForContrastiveLearning',\n",
       " 'BridgeTowerForImageAndTextRetrieval',\n",
       " 'BridgeTowerForMaskedLM',\n",
       " 'BridgeTowerModel',\n",
       " 'BridgeTowerPreTrainedModel',\n",
       " 'BROS_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BrosConfig',\n",
       " 'BrosProcessor',\n",
       " 'BROS_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BrosForTokenClassification',\n",
       " 'BrosModel',\n",
       " 'BrosPreTrainedModel',\n",
       " 'BrosSpadeEEForTokenClassification',\n",
       " 'BrosSpadeELForTokenClassification',\n",
       " 'ByT5Tokenizer',\n",
       " 'CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CamembertConfig',\n",
       " 'CamembertTokenizerFast',\n",
       " 'CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CamembertForCausalLM',\n",
       " 'CamembertForMaskedLM',\n",
       " 'CamembertForMultipleChoice',\n",
       " 'CamembertForQuestionAnswering',\n",
       " 'CamembertForSequenceClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CamembertModel',\n",
       " 'CamembertPreTrainedModel',\n",
       " 'CANINE_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CanineConfig',\n",
       " 'CanineTokenizer',\n",
       " 'CANINE_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CanineForMultipleChoice',\n",
       " 'CanineForQuestionAnswering',\n",
       " 'CanineForSequenceClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'CanineLayer',\n",
       " 'CanineModel',\n",
       " 'CaninePreTrainedModel',\n",
       " 'load_tf_weights_in_canine',\n",
       " 'CHINESE_CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ChineseCLIPConfig',\n",
       " 'ChineseCLIPProcessor',\n",
       " 'ChineseCLIPTextConfig',\n",
       " 'ChineseCLIPVisionConfig',\n",
       " 'ChineseCLIPFeatureExtractor',\n",
       " 'ChineseCLIPImageProcessor',\n",
       " 'CHINESE_CLIP_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ChineseCLIPModel',\n",
       " 'ChineseCLIPPreTrainedModel',\n",
       " 'ChineseCLIPTextModel',\n",
       " 'ChineseCLIPVisionModel',\n",
       " 'CLAP_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ClapAudioConfig',\n",
       " 'ClapConfig',\n",
       " 'ClapProcessor',\n",
       " 'ClapTextConfig',\n",
       " 'ClapAudioModel',\n",
       " 'ClapAudioModelWithProjection',\n",
       " 'ClapFeatureExtractor',\n",
       " 'ClapModel',\n",
       " 'ClapPreTrainedModel',\n",
       " 'ClapTextModel',\n",
       " 'ClapTextModelWithProjection',\n",
       " 'CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CLIPConfig',\n",
       " 'CLIPProcessor',\n",
       " 'CLIPTextConfig',\n",
       " 'CLIPTokenizer',\n",
       " 'CLIPVisionConfig',\n",
       " 'CLIPTokenizerFast',\n",
       " 'CLIPFeatureExtractor',\n",
       " 'CLIPImageProcessor',\n",
       " 'CLIP_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CLIPModel',\n",
       " 'CLIPPreTrainedModel',\n",
       " 'CLIPTextModel',\n",
       " 'CLIPTextModelWithProjection',\n",
       " 'CLIPVisionModel',\n",
       " 'CLIPVisionModelWithProjection',\n",
       " 'CLIPSEG_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CLIPSegConfig',\n",
       " 'CLIPSegProcessor',\n",
       " 'CLIPSegTextConfig',\n",
       " 'CLIPSegVisionConfig',\n",
       " 'CLIPSEG_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CLIPSegForImageSegmentation',\n",
       " 'CLIPSegModel',\n",
       " 'CLIPSegPreTrainedModel',\n",
       " 'CLIPSegTextModel',\n",
       " 'CLIPSegVisionModel',\n",
       " 'CLVP_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ClvpConfig',\n",
       " 'ClvpDecoderConfig',\n",
       " 'ClvpEncoderConfig',\n",
       " 'ClvpFeatureExtractor',\n",
       " 'ClvpProcessor',\n",
       " 'ClvpTokenizer',\n",
       " 'CLVP_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ClvpDecoder',\n",
       " 'ClvpEncoder',\n",
       " 'ClvpForCausalLM',\n",
       " 'ClvpModel',\n",
       " 'ClvpModelForConditionalGeneration',\n",
       " 'ClvpPreTrainedModel',\n",
       " 'CodeLlamaTokenizerFast',\n",
       " 'CODEGEN_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CodeGenConfig',\n",
       " 'CodeGenTokenizer',\n",
       " 'CodeGenTokenizerFast',\n",
       " 'CODEGEN_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CodeGenForCausalLM',\n",
       " 'CodeGenModel',\n",
       " 'CodeGenPreTrainedModel',\n",
       " 'CONDITIONAL_DETR_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ConditionalDetrConfig',\n",
       " 'ConditionalDetrFeatureExtractor',\n",
       " 'ConditionalDetrImageProcessor',\n",
       " 'CONDITIONAL_DETR_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ConditionalDetrForObjectDetection',\n",
       " 'ConditionalDetrForSegmentation',\n",
       " 'ConditionalDetrModel',\n",
       " 'ConditionalDetrPreTrainedModel',\n",
       " 'CONVBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ConvBertConfig',\n",
       " 'ConvBertTokenizer',\n",
       " 'ConvBertTokenizerFast',\n",
       " 'CONVBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ConvBertForMaskedLM',\n",
       " 'ConvBertForMultipleChoice',\n",
       " 'ConvBertForQuestionAnswering',\n",
       " 'ConvBertForSequenceClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'ConvBertLayer',\n",
       " 'ConvBertModel',\n",
       " 'ConvBertPreTrainedModel',\n",
       " 'load_tf_weights_in_convbert',\n",
       " 'CONVNEXT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ConvNextConfig',\n",
       " 'ConvNextFeatureExtractor',\n",
       " 'ConvNextImageProcessor',\n",
       " 'CONVNEXT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ConvNextBackbone',\n",
       " 'ConvNextForImageClassification',\n",
       " 'ConvNextModel',\n",
       " 'ConvNextPreTrainedModel',\n",
       " 'CONVNEXTV2_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ConvNextV2Config',\n",
       " 'CONVNEXTV2_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ConvNextV2Backbone',\n",
       " 'ConvNextV2ForImageClassification',\n",
       " 'ConvNextV2Model',\n",
       " 'ConvNextV2PreTrainedModel',\n",
       " 'CpmTokenizerFast',\n",
       " 'CPMANT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CpmAntConfig',\n",
       " 'CpmAntTokenizer',\n",
       " 'CPMANT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CpmAntForCausalLM',\n",
       " 'CpmAntModel',\n",
       " 'CpmAntPreTrainedModel',\n",
       " 'CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CTRLConfig',\n",
       " 'CTRLTokenizer',\n",
       " 'CTRL_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CTRLForSequenceClassification',\n",
       " 'CTRLLMHeadModel',\n",
       " 'CTRLModel',\n",
       " 'CTRLPreTrainedModel',\n",
       " 'CVT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CvtConfig',\n",
       " 'CVT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CvtForImageClassification',\n",
       " 'CvtModel',\n",
       " 'CvtPreTrainedModel',\n",
       " 'DATA2VEC_TEXT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DATA2VEC_VISION_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'Data2VecAudioConfig',\n",
       " 'Data2VecTextConfig',\n",
       " 'Data2VecVisionConfig',\n",
       " 'DATA2VEC_AUDIO_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DATA2VEC_TEXT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DATA2VEC_VISION_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'Data2VecAudioForAudioFrameClassification',\n",
       " 'Data2VecAudioForCTC',\n",
       " 'Data2VecAudioForSequenceClassification',\n",
       " 'Data2VecAudioForXVector',\n",
       " 'Data2VecAudioModel',\n",
       " 'Data2VecAudioPreTrainedModel',\n",
       " 'Data2VecTextForCausalLM',\n",
       " 'Data2VecTextForMaskedLM',\n",
       " 'Data2VecTextForMultipleChoice',\n",
       " 'Data2VecTextForQuestionAnswering',\n",
       " 'Data2VecTextForSequenceClassification',\n",
       " 'Data2VecTextForTokenClassification',\n",
       " 'Data2VecTextModel',\n",
       " 'Data2VecTextPreTrainedModel',\n",
       " 'Data2VecVisionForImageClassification',\n",
       " 'Data2VecVisionForSemanticSegmentation',\n",
       " 'Data2VecVisionModel',\n",
       " 'Data2VecVisionPreTrainedModel',\n",
       " 'DEBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DebertaConfig',\n",
       " 'DebertaTokenizer',\n",
       " 'DebertaTokenizerFast',\n",
       " 'DEBERTA_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DebertaForMaskedLM',\n",
       " 'DebertaForQuestionAnswering',\n",
       " 'DebertaForSequenceClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaModel',\n",
       " 'DebertaPreTrainedModel',\n",
       " 'DEBERTA_V2_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DebertaV2Config',\n",
       " 'DebertaV2TokenizerFast',\n",
       " 'DEBERTA_V2_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DebertaV2ForMaskedLM',\n",
       " 'DebertaV2ForMultipleChoice',\n",
       " 'DebertaV2ForQuestionAnswering',\n",
       " 'DebertaV2ForSequenceClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DebertaV2Model',\n",
       " 'DebertaV2PreTrainedModel',\n",
       " 'DECISION_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DecisionTransformerConfig',\n",
       " 'DECISION_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DecisionTransformerGPT2Model',\n",
       " 'DecisionTransformerGPT2PreTrainedModel',\n",
       " 'DecisionTransformerModel',\n",
       " 'DecisionTransformerPreTrainedModel',\n",
       " 'DEFORMABLE_DETR_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DeformableDetrConfig',\n",
       " 'DeformableDetrFeatureExtractor',\n",
       " 'DeformableDetrImageProcessor',\n",
       " 'DEFORMABLE_DETR_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DeformableDetrForObjectDetection',\n",
       " 'DeformableDetrModel',\n",
       " 'DeformableDetrPreTrainedModel',\n",
       " 'DEIT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DeiTConfig',\n",
       " 'DeiTFeatureExtractor',\n",
       " 'DeiTImageProcessor',\n",
       " 'DEIT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DeiTForImageClassification',\n",
       " 'DeiTForImageClassificationWithTeacher',\n",
       " 'DeiTForMaskedImageModeling',\n",
       " 'DeiTModel',\n",
       " 'DeiTPreTrainedModel',\n",
       " 'MCTCT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'MCTCTConfig',\n",
       " 'MCTCTFeatureExtractor',\n",
       " 'MCTCTProcessor',\n",
       " 'MCTCT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'MCTCTForCTC',\n",
       " 'MCTCTModel',\n",
       " 'MCTCTPreTrainedModel',\n",
       " 'MMBTConfig',\n",
       " 'MMBTForClassification',\n",
       " 'MMBTModel',\n",
       " 'ModalEmbeddings',\n",
       " 'OPEN_LLAMA_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'OpenLlamaConfig',\n",
       " 'OpenLlamaForCausalLM',\n",
       " 'OpenLlamaForSequenceClassification',\n",
       " 'OpenLlamaModel',\n",
       " 'OpenLlamaPreTrainedModel',\n",
       " 'RETRIBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'RetriBertConfig',\n",
       " 'RetriBertTokenizer',\n",
       " 'RetriBertTokenizerFast',\n",
       " 'RETRIBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'RetriBertModel',\n",
       " 'RetriBertPreTrainedModel',\n",
       " 'TapexTokenizer',\n",
       " 'TRAJECTORY_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'TrajectoryTransformerConfig',\n",
       " 'TRAJECTORY_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'TrajectoryTransformerModel',\n",
       " 'TrajectoryTransformerPreTrainedModel',\n",
       " 'VAN_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'VanConfig',\n",
       " 'VAN_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'VanForImageClassification',\n",
       " 'VanModel',\n",
       " 'VanPreTrainedModel',\n",
       " 'DETA_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DetaConfig',\n",
       " 'DetaImageProcessor',\n",
       " 'DETA_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DetaForObjectDetection',\n",
       " 'DetaModel',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "transformers.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbaa5237-8a06-4eae-aa27-ea8337262eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1881/1881 [00:00<00:00, 3042609.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 1, 5, 5, 5, 3, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 1, 4, 2, 2, 2, 1, 2, 3, 1, 4, 2, 2, 1, 2, 1, 2, 4, 1, 5, 3, 2, 5, 2, 1, 2, 4, 1, 2, 2, 2, 4, 5, 1, 4, 5, 3, 4, 2, 2, 4, 5, 1, 2, 2, 5, 4, 2, 2, 3, 2, 2, 2, 5, 2, 1, 4, 2, 2, 5, 2, 5, 5, 2, 1, 4, 1, 5, 3, 5, 1, 4, 4, 1, 4, 3, 4, 4, 1, 4, 4, 5, 2, 3, 1, 3, 2, 2, 2, 4, 2, 3, 5, 1, 3, 2, 2, 1, 1, 2, 2, 5, 4, 2, 2, 2, 2, 4, 3, 3, 1, 4, 3, 1, 4, 2, 2, 1, 2, 5, 3, 4, 4, 2, 1, 4, 2, 3, 3, 2, 2, 4, 4, 1, 2, 4, 3, 5, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 3, 2, 3, 5, 3, 4, 2, 2, 5, 5, 2, 1, 3, 1, 4, 4, 2, 4, 4, 3, 2, 2, 2, 1, 1, 4, 2, 3, 2, 2, 2, 5, 1, 1, 4, 2, 3, 2, 5, 4, 1, 2, 5, 4, 2, 2, 3, 2, 1, 1, 2, 2, 1, 2, 2, 4, 2, 3, 5, 5, 2, 2, 2, 5, 1, 2, 5, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 2, 1, 2, 2, 1, 4, 2, 2, 4, 2, 4, 2, 2, 2, 4, 1, 1, 2, 1, 4, 4, 2, 2, 3, 4, 2, 2, 2, 4, 5, 2, 5, 3, 2, 2, 4, 4, 5, 4, 1, 5, 2, 5, 3, 1, 4, 2, 2, 2, 3, 2, 4, 2, 1, 4, 2, 2, 2, 2, 2, 2, 5, 1, 2, 5, 2, 3, 2, 2, 4, 2, 1, 4, 4, 2, 2, 2, 4, 2, 5, 3, 3, 4, 4, 2, 2, 2, 2, 1, 5, 5, 2, 2, 2, 2, 2, 5, 5, 2, 4, 1, 1, 2, 5, 3, 3, 1, 5, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 5, 4, 4, 2, 2, 2, 2, 5, 5, 2, 2, 2, 3, 5, 2, 5, 2, 2, 3, 2, 4, 4, 5, 4, 5, 5, 2, 5, 2, 2, 2, 3, 1, 3, 5, 2, 4, 2, 1, 3, 5, 3, 3, 2, 1, 5, 4, 4, 2, 5, 2, 2, 3, 2, 2, 2, 3, 4, 4, 3, 2, 2, 4, 2, 4, 2, 2, 3, 4, 2, 2, 2, 3, 2, 5, 2, 2, 2, 4, 2, 2, 4, 1, 4, 4, 2, 2, 2, 2, 3, 2, 2, 1, 4, 2, 2, 2, 1, 2, 2, 2, 3, 4, 5, 1, 2, 2, 3, 2, 3, 5, 1, 4, 5, 2, 2, 5, 4, 1, 2, 5, 4, 3, 1, 2, 4, 4, 2, 4, 2, 1, 2, 4, 2, 2, 1, 2, 2, 5, 4, 2, 4, 4, 4, 2, 1, 3, 1, 2, 3, 2, 5, 1, 1, 2, 2, 2, 2, 2, 4, 2, 5, 4, 4, 2, 5, 2, 2, 2, 2, 4, 2, 5, 4, 2, 4, 2, 2, 4, 4, 5, 2, 3, 2, 1, 2, 2, 5, 2, 5, 2, 2, 3, 2, 2, 2, 2, 2, 2, 1, 5, 4, 2, 2, 3, 2, 1, 4, 5, 2, 2, 3, 4, 4, 1, 2, 2, 4, 4, 4, 2, 3, 3, 2, 2, 2, 5, 2, 5, 2, 3, 2, 2, 3, 2, 4, 4, 2, 2, 5, 2, 4, 2, 2, 3, 4, 2, 2, 2, 3, 1, 2, 5, 2, 2, 1, 1, 5, 4, 2, 1, 5, 2, 2, 2, 5, 1, 4, 2, 4, 5, 2, 2, 2, 2, 4, 5, 1, 3, 5, 3, 4, 3, 2, 4, 2, 2, 2, 1, 4, 2, 4, 3, 4, 2, 2, 3, 2, 5, 2, 2, 2, 4, 3, 2, 2, 2, 4, 1, 2, 5, 1, 4, 2, 2, 2, 2, 4, 2, 2, 5, 2, 4, 2, 4, 4, 2, 4, 1, 1, 2, 2, 2, 2, 1, 2, 4, 4, 2, 2, 3, 2, 2, 2, 2, 2, 2, 1, 5, 3, 1, 2, 2, 2, 2, 4, 2, 2, 2, 3, 2, 2, 5, 2, 4, 4, 5, 2, 3, 2, 2, 2, 2, 2, 1, 3, 4, 3, 2, 3, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 4, 2, 1, 2, 2, 4, 2, 4, 2, 2, 3, 3, 3, 5, 2, 5, 2, 3, 3, 5, 2, 1, 2, 1, 2, 3, 5, 2, 2, 3, 2, 5, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 4, 2, 5, 2, 2, 2, 3, 4, 1, 1, 3, 2, 4, 5, 2, 5, 3, 5, 5, 2, 5, 5, 3, 5, 5, 2, 2, 4, 2, 2, 5, 4, 4, 4, 2, 4, 2, 2, 4, 1, 2, 5, 2, 2, 4, 2, 3, 1, 2, 4, 2, 5, 2, 3, 5, 2, 2, 4, 1, 2, 2, 2, 1, 2, 5, 1, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 2, 1, 4, 2, 2, 5, 4, 1, 3, 5, 2, 2, 2, 4, 3, 4, 2, 2, 2, 5, 5, 3, 2, 2, 2, 2, 2, 4, 5, 3, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 2, 5, 1, 1, 4, 3, 2, 3, 4, 2, 2, 1, 2, 5, 1, 3, 3, 4, 2, 4, 3, 3, 2, 5, 4, 2, 4, 2, 1, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 3, 5, 2, 3, 0, 2, 2, 2, 2, 3, 2, 3, 2, 5, 1, 2, 2, 3, 5, 2, 2, 2, 2, 5, 4, 2, 4, 1, 2, 2, 1, 2, 0, 4, 5, 2, 2, 1, 2, 1, 2, 5, 2, 2, 4, 2, 2, 2, 4, 2, 5, 2, 1, 2, 2, 4, 2, 3, 2, 3, 2, 2, 4, 2, 2, 2, 2, 2, 2, 1, 3, 2, 1, 2, 3, 2, 2, 2, 2, 5, 2, 4, 3, 4, 2, 2, 2, 1, 4, 1, 2, 1, 5, 3, 4, 1, 4, 3, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 3, 3, 1, 4, 2, 3, 1, 4, 1, 1, 2, 2, 3, 1, 4, 5, 5, 4, 2, 2, 2, 3, 2, 5, 3, 5, 2, 4, 1, 3, 2, 1, 2, 2, 2, 4, 5, 4, 5, 3, 2, 2, 2, 3, 2, 2, 3, 4, 2, 4, 2, 2, 2, 1, 3, 3, 1, 4, 5, 2, 3, 3, 2, 2, 2, 2, 4, 5, 2, 2, 4, 5, 4, 3, 5, 2, 5, 4, 2, 2, 2, 4, 1, 4, 4, 2, 2, 4, 4, 4, 3, 5, 2, 2, 4, 4, 5, 4, 1, 2, 1, 2, 2, 3, 1, 2, 4, 4, 3, 2, 2, 2, 4, 2, 2, 4, 2, 1, 2, 2, 2, 1, 2, 4, 2, 5, 2, 2, 2, 5, 4, 3, 0, 2, 4, 5, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 1, 2, 3, 3, 2, 2, 5, 5, 4, 1, 2, 5, 4, 1, 2, 3, 2, 2, 2, 2, 2, 2, 5, 4, 2, 2, 5, 4, 2, 3, 2, 3, 5, 2, 2, 5, 1, 2, 3, 2, 2, 3, 2, 2, 2, 1, 2, 3, 4, 4, 5, 5, 3, 3, 4, 2, 2, 2, 1, 4, 2, 3, 2, 3, 3, 2, 5, 2, 2, 5, 2, 4, 3, 4, 2, 2, 5, 1, 2, 2, 5, 5, 4, 2, 4, 4, 2, 3, 2, 1, 2, 2, 4, 2, 2, 4, 2, 2, 2, 5, 2, 2, 3, 1, 3, 3, 1, 3, 3, 1, 2, 2, 2, 3, 2, 4, 2, 4, 2, 1, 2, 2, 2, 1, 5, 2, 2, 1, 1, 3, 2, 2, 4, 2, 1, 2, 4, 4, 5, 2, 1, 2, 5, 5, 2, 2, 4, 3, 2, 4, 3, 2, 2, 1, 2, 2, 2, 2, 3, 5, 5, 3, 2, 3, 2, 5, 2, 2, 1, 2, 4, 4, 5, 4, 2, 2, 2, 2, 5, 2, 2, 5, 1, 3, 1, 5, 4, 2, 5, 2, 3, 5, 4, 3, 2, 3, 2, 2, 2, 2, 5, 2, 1, 4, 2, 4, 1, 1, 2, 2, 2, 3, 5, 2, 5, 5, 3, 5, 2, 4, 3, 4, 3, 4, 2, 1, 4, 5, 2, 4, 5, 2, 2, 2, 2, 3, 2, 5, 2, 1, 2, 2, 4, 1, 2, 2, 2, 4, 3, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 4, 5, 2, 2, 4, 5, 5, 4, 5, 2, 2, 2, 5, 2, 4, 2, 4, 5, 2, 4, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 5, 2, 5, 2, 2, 2, 5, 5, 2, 1, 1, 2, 4, 2, 1, 2, 2, 1, 3, 4, 1, 2, 5, 2, 2, 4, 1, 2, 4, 5, 0, 3, 2, 2, 2, 2, 2, 4, 1, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 1, 4, 3, 1, 3, 4, 4, 2, 2, 4, 2, 2, 2, 2, 1, 5, 2, 5, 1, 2, 2, 3, 5, 3, 4, 2, 1, 2, 4, 2, 5, 2, 2, 1, 2, 5, 2, 4, 5, 2, 2, 2, 2, 4, 3, 2, 2, 2, 1, 1, 3, 2, 2, 4, 1, 1, 2, 2, 4, 2, 2, 4, 2, 2, 1, 2, 4, 2, 4, 4, 2, 3, 2, 3, 4, 3, 4, 2, 5, 2, 3, 0, 5, 5, 2, 3, 4, 4, 2, 2, 2, 3, 2, 3, 5, 3, 4, 2, 4, 4, 3, 4, 2, 2, 5, 4, 4, 2, 3, 5, 1, 1, 2, 5, 4, 5, 2, 2, 1, 2, 4, 4, 4, 2, 3, 1, 3, 2, 2, 1, 1, 4, 1, 4, 2, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 2, 1, 5, 5, 4, 2, 2, 2, 1, 5, 3, 2, 3, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 4, 3, 5, 4, 2, 3, 2, 2, 4, 2, 4, 2, 2, 3, 2, 2, 3, 2, 3, 2, 4, 2, 4, 2, 2, 5, 1, 5, 2, 1, 2, 4, 2, 4, 4, 3, 4, 2, 1, 3, 2, 5, 5, 2, 4, 2, 2, 5, 1, 2, 5, 5, 3, 2, 1, 3, 2, 4, 5, 5, 2, 1, 2, 5, 5, 4, 2, 2, 2, 4, 1, 2, 3, 2, 2, 2, 3, 2, 2, 4, 2, 5, 1, 5, 1, 2, 3, 4, 4, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 2, 3, 2, 4, 2, 3, 2, 2, 2, 1, 2, 2, 4, 2, 2, 2, 5, 3, 3, 3, 4, 2, 1, 2, 2, 3, 2, 3, 2, 5, 2, 1, 1, 3, 4, 2, 4, 5, 1, 2, 2, 3, 4, 5, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "dic = {\n",
    "\"ANGRY\":0,\n",
    "\"FEAR\":1,\n",
    "\"SAD\":2,\n",
    "\"DISGUST\":3,\n",
    "\"NEUTRAL\":4,\n",
    "\n",
    "\"HAPPY\":5\n",
    "\n",
    "}\n",
    "submission = pd.read_csv(\"/scratch/network/mk8574/audio_sentiment_challenge/baseline_dy/test_submission.csv\")\n",
    "preds = []\n",
    "for i in tqdm(submission[\"label\"]):\n",
    "    preds.append(dic[i])\n",
    "print(preds)\n",
    "submission['label'] = preds\n",
    "submission.to_csv(\"/scratch/network/mk8574/audio_sentiment_challenge/baseline_dy/test_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e886e7-555a-48b5-ac33-d68398071f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk8574_3.10 [~/.conda/envs/mk8574_3.10/]",
   "language": "python",
   "name": "conda_mk8574_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
