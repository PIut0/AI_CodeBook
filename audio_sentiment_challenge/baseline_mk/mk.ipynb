{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9081c53e-7e99-4909-9007-919f7072be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from dataclasses import asdict, dataclass\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import librosa\n",
    "import audiomentations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor, AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d31d224-cb01-4d9d-84c5-8b980bfd53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    device: int = 1\n",
    "    # data args\n",
    "    data_path: str = \"/scratch/network/mk8574/audio_sentiment_challenge/data\"\n",
    "    val_size: float = 0.2\n",
    "    \n",
    "    train_transforms: audiomentations.OneOf(\n",
    "    [\n",
    "        audiomentations.AddGaussianNoise(p=0.75),\n",
    "        audiomentations.PitchShift(p=0.75),\n",
    "        audiomentations.PeakingFilter(p=0.75),\n",
    "        audiomentations.SevenBandParametricEQ(p=0.75),\n",
    "        audiomentations.BandPassFilter(p=0.75),\n",
    "        audiomentations.BandStopFilter(p=0.75),\n",
    "        audiomentations.AirAbsorption(p=0.75),\n",
    "        audiomentations.ClippingDistortion(p=0.75),\n",
    "        audiomentations.HighPassFilter(p=0.75),\n",
    "        audiomentations.HighShelfFilter(p=0.75),\n",
    "        audiomentations.Limiter(p=0.75),\n",
    "        audiomentations.LowPassFilter(p=0.75),\n",
    "        audiomentations.LowShelfFilter(p=0.75),\n",
    "    ]) = None\n",
    "    \n",
    "    # save dir\n",
    "    train_serial = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir: str = f\"/scratch/network/mk8574/audio_sentiment_challenge/baseline_mk/results/{train_serial}\"\n",
    "\n",
    "    # model args\n",
    "    pretrained_name: str = \"openai/whisper-base\"\n",
    "\n",
    "    # hparams\n",
    "    seed: int = 42\n",
    "    max_epoch: int = 32\n",
    "    lr: float = 5e-4\n",
    "    batch_size: int = 8\n",
    "    total_batch_size: int = 32\n",
    "    gradient_accumulate_step: int = 4  # total batch size = batch_size * gradient_accumulate_step\n",
    "    early_stop_patience = 5\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ee1b10e-627b-4c1a-a26b-58f51a18a09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f'cuda:{config.device}') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5e8cb0e-604c-4742-b770-c4e7accfc0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = AutoProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "# model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "137033d4-dfe2-4ec8-bf71-e0edaeb0a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(config.save_dir):\n",
    "#     os.makedirs(config.save_dir)\n",
    "\n",
    "# with open(os.path.join(config.save_dir, \"config.json\"), \"w\") as config_file:\n",
    "#     json.dump(asdict(config), config_file, indent=4, sort_keys=False)\n",
    "# print(f'config file saved at: {config.train_serial}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eb4a6b5-7304-4e34-905e-e05029bf27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(config.seed) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc5ad5e2-d35a-4eac-a775-bdc2c1e7a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(config.data_path, 'train.csv'))\n",
    "train_df, valid_df = train_test_split(train_df, test_size=config.val_size, random_state=config.seed)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c624f74-810d-4a23-ae7b-d15c55520261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, df, feature_extractor, mode='train', transforms=None):\n",
    "        self.df = df\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(config.data_path, self.df['path'][idx][2:])\n",
    "        \n",
    "        waveform, sample_rate = librosa.load(path)\n",
    "        sr = self.feature_extractor.sampling_rate\n",
    "        waveform = librosa.resample(waveform, orig_sr=sample_rate, target_sr=sr)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            waveform = self.transforms(samples=np.array(waveform, dtype=np.float32), sample_rate=sr)\n",
    "        \n",
    "        input_values = self.feature_extractor(waveform, sampling_rate=sr, return_tensors=\"pt\", padding=True).input_values\n",
    "        \n",
    "        if self.mode is not 'test':\n",
    "            label = self.df['label'][idx]\n",
    "            return input_values.squeeze(), label\n",
    "        else:\n",
    "            return input_values.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7949e8b-75c2-4022-9b37-43446ff33190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_yes_label(batch):\n",
    "    waveforms, labels = zip(*batch)\n",
    "    waveforms = pad_sequence([torch.tensor(wave) for wave in waveforms], batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    return waveforms, labels\n",
    "\n",
    "def collate_fn_no_label(batch):\n",
    "    waveforms = zip(*batch)\n",
    "    waveforms = pad_sequence([torch.tensor(wave) for wave in waveforms], batch_first=True)\n",
    "    return waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89209b52-b423-44a9-906b-f6a1de13b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(config.pretrained_name)\n",
    "\n",
    "train_dataset = MyDataSet(train_df, feature_extractor, mode='train', transforms=config.train_transforms)\n",
    "valid_dataset = MyDataSet(valid_df, feature_extractor, mode='val')\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=config.batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn_yes_label,\n",
    "                          num_workers=16)\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                          batch_size=config.batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn_yes_label,\n",
    "                          num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69124094-fc1b-40d5-905a-7c0c227a8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_name: str):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = AutoModelForAudioClassification.from_pretrained(pretrained_name)\n",
    "        \n",
    "        self.model.classifier = nn.Linear(in_features=self.model.projector.out_features, out_features=6)\n",
    "        nn.init.kaiming_normal_(self.model.classifier.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        nn.init.zeros_(self.model.classifier.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f5ac3-2e44-4ef1-8213-0c085704be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, creterion):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for waveforms, labels in tqdm(iter(valid_loader)):\n",
    "            waveforms = waveforms.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(waveforms)            \n",
    "            loss = creterion(output, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).cpu().sum()\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    avg_loss = np.mean(val_loss)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1828d9e-12bb-4d4a-b5a4-e7cdd8593097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, optimizer, scheduler):\n",
    "    accumulation_step = int(config.total_batch_size / config.batch_size)\n",
    "    model.to(device)\n",
    "    creterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(1, config.max_epoch+1):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        \n",
    "        for i, (waveforms, labels) in enumerate(tqdm(train_loader)):\n",
    "            waveforms = waveforms.to(device)\n",
    "            labels = labels.flatten().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(waveforms)\n",
    "            loss = creterion(output, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            if (i+1) % accumulation_step == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        avg_loss = np.mean(train_loss)\n",
    "        valid_loss, valid_acc = validation(model, valid_loader, creterion)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_model = model\n",
    "\n",
    "        print(f'epoch:[{epoch}] train loss:[{avg_loss:.5f}] valid_loss:[{valid_loss:.5f}] valid_acc:[{valid_acc:.5f}]')\n",
    "    \n",
    "    print(f'best_acc:{best_acc:.5f}')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e51a7-4491-4728-bb70-fe5ea0b9c06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at openai/whisper-base and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/500 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 92, in __getattr__\n    return self.data[item]\nKeyError: 'input_values'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_1190954/1090693024.py\", line 21, in __getitem__\n    input_values = self.feature_extractor(waveform, sampling_rate=sr, return_tensors=\"pt\", padding=True).input_values\n  File \"/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 94, in __getattr__\n    raise AttributeError\nAttributeError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[1;32m      4\u001b[0m     [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: module\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m config\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m} \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_children()],\n\u001b[1;32m      5\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     10\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (waveforms, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[1;32m     14\u001b[0m     waveforms \u001b[38;5;241m=\u001b[39m waveforms\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 92, in __getattr__\n    return self.data[item]\nKeyError: 'input_values'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_1190954/1090693024.py\", line 21, in __getitem__\n    input_values = self.feature_extractor(waveform, sampling_rate=sr, return_tensors=\"pt\", padding=True).input_values\n  File \"/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 94, in __getattr__\n    raise AttributeError\nAttributeError\n"
     ]
    }
   ],
   "source": [
    "model = MyModel(config.pretrained_name)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [{\"params\": module.parameters(), \"lr\": config.lr if name == \"classifier\" else config.lr * 0.1} for name, module in model.named_children()],\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "best_model = train(model, train_loader, valid_loader, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa281d06-6981-4860-b89b-e49035b1f172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276dba66-3876-4e15-8e05-ae1e2cadde6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48c573-cf47-4517-883d-1c1dbb375d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324faf2-8c52-43c2-a610-3f8736a86fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a970b73-1154-4131-a0bb-81f0bcdd6702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a2de6-05ec-43c6-8a74-25520e8c44a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48f606-d9d8-4bcb-b179-722893a2a860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd7d97-14a1-4e06-82b5-553f1a1068d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf1dc3-8286-46c2-ba6e-692b50f23f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk8574_3.10 [~/.conda/envs/mk8574_3.10/]",
   "language": "python",
   "name": "conda_mk8574_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
