{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab1932a-964f-42a4-8a1b-dd232d360bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from dataclasses import asdict, dataclass\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "from datetime import datetime\n",
    "import audiomentations\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, AutoTokenizer\n",
    "from transformers.feature_extraction_utils import BatchFeature\n",
    "from transformers import AutoFeatureExtractor, WhisperForAudioClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234cfdc5-bbfa-40bc-8ff6-43f1071e7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # data args\n",
    "    train_csv: str = \"/scratch/network/mk8574/audio_sentiment_challenge/data/train.csv\"\n",
    "    test_csv: str = \"/scratch/network/mk8574/audio_sentiment_challenge/data/test.csv\"\n",
    "\n",
    "    # model args\n",
    "    pretrained_name: str = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "    \n",
    "    train_serial = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"|\" + pretrained_name.replace(\"/\", \"|\")\n",
    "\n",
    "    # k-fold\n",
    "    k_fold_num: int = 0  # if you want to use k-fold validation, set positive integer value.\n",
    "    k_fold_idx: int = 1\n",
    "\n",
    "    # save dir\n",
    "    save_dir: str = f\"/scratch/network/mk8574/audio_sentiment_challenge/baseline_mk/results/{train_serial}/\"\n",
    "\n",
    "    # hparams\n",
    "    seed: int = 42\n",
    "    lr: float = 5e-4\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulate_step: int = 4  # total batch size = batch_size * gradient_accumulate_step\n",
    "    max_epoch: int = batch_size * gradient_accumulate_step\n",
    "    early_stop_patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6fe578-f53f-49d7-94ef-12fe48050860",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "if not os.path.exists(config.save_dir):\n",
    "    os.makedirs(config.save_dir)\n",
    "\n",
    "with open(os.path.join(config.save_dir, \"config.json\"), \"w\") as config_file:\n",
    "    json.dump(asdict(config), config_file, indent=4, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73a817c-f6b2-49ea-8f4a-eafc0ac86524",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")\n",
    "model = WhisperForAudioClassification.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd00d23-78ee-49ea-87e4-a61180100f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = config.seed\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7923ed56-0586-44bf-b470-f881b509e179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 5002/5002 [00:00<00:00, 626191.16it/s]\n",
      "Resolving data files: 100%|██████████| 1882/1882 [00:00<00:00, 607439.79it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"audiofolder\", data_dir = '../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60960537-4261-4086-b704-f04526fad8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'label'],\n",
       "    num_rows: 5001\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ed39c6-68fe-409f-aad7-dfe042c5fef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/scratch/network/mk8574/audio_sentiment_challenge/data/train/TRAIN_0000.wav', 'array': array([0.00750732, 0.00820923, 0.00793457, ..., 0.        , 0.        ,\n",
      "       0.        ]), 'sampling_rate': 16000}, 'label': None}\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(ds['train']))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098c4fac-b6ad-46cd-9a9e-6039938ec4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37904"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample['audio']['array'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a01dd4d3-2a83-4e44-afad-2210276816ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3318,  0.4619,  0.3513,  ..., -1.2950, -1.2950, -1.2950],\n",
      "         [ 0.0703,  0.2434,  0.3929,  ..., -1.2950, -1.2950, -1.2950],\n",
      "         [ 0.4198,  0.3761,  0.4405,  ..., -1.2950, -1.2950, -1.2950],\n",
      "         ...,\n",
      "         [-0.5193, -0.6057, -0.6061,  ..., -1.2950, -1.2950, -1.2950],\n",
      "         [-0.5937, -0.6416, -0.6173,  ..., -1.2950, -1.2950, -1.2950],\n",
      "         [-0.5799, -0.6215, -0.5754,  ..., -1.2950, -1.2950, -1.2950]]])\n"
     ]
    }
   ],
   "source": [
    "inputs = feature_extractor(sample['audio']['array'], sampling_rate = sample['audio']['sampling_rate'])\n",
    "\n",
    "input_features = torch.Tensor(np.array(inputs.input_features))\n",
    "print(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02ef7516-2d77-4483-81de-75c4d70efce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hebrew'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(input_features).logits\n",
    "\n",
    "predicted_class_ids = torch.argmax(logits).item()\n",
    "predicted_label = model.config.id2label[predicted_class_ids]\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e30cba-6dae-446a-a156-b90f464d763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label = {i:i for i in range(6)}\n",
    "model.config.label2id = model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baa36b07-4da9-4798-992f-e9d11469b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.num_labels = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b448d6a2-ff78-4e04-baf4-71c1352e84e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/mk8574/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Nov 22 21:05:41 2023) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "training_args = TrainingArguments(output_dir = \"test_trainer\", report_to = 'none')\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis = -1)\n",
    "    \n",
    "    return metric.compute(predictions = predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c445c2b-04bb-49ed-a4b0-a3d728d3a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, df, feature_extractor, mode='train', transforms=None):\n",
    "        self.df = df\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join('../data', self.df['path'][idx][2:])\n",
    "        \n",
    "        waveform, sample_rate = librosa.load(path)\n",
    "        sr = self.feature_extractor.sampling_rate\n",
    "        if sr != sample_rate:\n",
    "            waveform = librosa.resample(waveform, orig_sr=sample_rate, target_sr=sr)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            waveform = self.transforms(samples=np.array(waveform, dtype=np.float32), sample_rate=sr)\n",
    "        \n",
    "        input_values = self.feature_extractor(torch.Tensor(waveform), sampling_rate=sr, return_tensors=\"pt\", padding=True).input_features\n",
    "        \n",
    "        if self.mode != 'test':\n",
    "            label = self.df['label'][idx]\n",
    "            return input_values.squeeze(), label\n",
    "        else:\n",
    "            return input_values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40579bb3-52a1-43b4-a069-b0c6c456d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_loader = MyDataSet(train_df, feature_extractor)\n",
    "valid_loader = MyDataSet(valid_df, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c702a95-c9ee-4c01-9c33-3c0d98139457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f7b91df-cdf8-418d-832b-5e890f64137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor: AutoFeatureExtractor,\n",
    "        transforms: list = None,\n",
    "    ):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def to_dataset(self, df: pd.DataFrame) -> Dataset:\n",
    "        def load_waveform(row):\n",
    "            waveform, sample_rate = librosa.load(row[\"path\"])\n",
    "            waveform = librosa.resample(\n",
    "                waveform,\n",
    "                orig_sr=sample_rate,\n",
    "                target_sr=self.feature_extractor.sampling_rate,\n",
    "            )\n",
    "            row[\"waveform\"] = waveform\n",
    "\n",
    "            return row\n",
    "\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        dataset = dataset.map(load_waveform, num_proc=4)\n",
    "        if \"label\" in dataset.column_names:\n",
    "            dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def apply_transforms(self, dataset: Dataset) -> Dataset:\n",
    "        def apply_transforms(batch):\n",
    "            waveforms = [self.transforms(samples=np.array(waveform, dtype=np.float32), sample_rate=self.feature_extractor.sampling_rate) for waveform in batch[\"waveform\"]]\n",
    "            batch[\"waveform\"] = waveforms\n",
    "\n",
    "            return batch\n",
    "\n",
    "        if self.transforms:\n",
    "            dataset = dataset.with_transform(apply_transforms)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def collate_fn(self, batch: list) -> BatchFeature:\n",
    "        if hasattr(self.feature_extractor, \"nb_max_frames\"):\n",
    "            padding = \"max_length\"\n",
    "        else:\n",
    "            padding = \"longest\"\n",
    "\n",
    "        waveforms = [data[\"waveform\"] for data in batch]\n",
    "        model_inputs = self.feature_extractor(\n",
    "            waveforms,\n",
    "            sampling_rate=self.feature_extractor.sampling_rate,\n",
    "            padding=padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if \"labels\" in batch[0]:\n",
    "            labels = [data[\"labels\"] for data in batch]\n",
    "            model_inputs[\"labels\"] = torch.tensor(labels)\n",
    "\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21f2ffb5-91f1-452e-9821-f0079c33353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricScore(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        metrics = MetricCollection(\n",
    "            {\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=6),\n",
    "                \"recall\": MulticlassRecall(num_classes=6),\n",
    "                \"precision\": MulticlassPrecision(num_classes=6),\n",
    "                \"f1\": MulticlassF1Score(num_classes=6),\n",
    "            }\n",
    "        )\n",
    "        self.train_metrics = metrics.clone()\n",
    "        self.valid_metrics = metrics.clone()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "\n",
    "    def add_train_metrics(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        self.train_metrics.update(logits, labels)\n",
    "\n",
    "    def add_valid_metrics(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        self.valid_metrics.update(logits, labels)\n",
    "\n",
    "    def add_train_loss(self, loss: torch.Tensor):\n",
    "        self.train_losses.append(loss.item())\n",
    "\n",
    "    def add_valid_loss(self, loss: torch.Tensor):\n",
    "        self.valid_losses.append(loss.item())\n",
    "\n",
    "    def compute_train(self) -> Dict[str, Any]:\n",
    "        scores = self.train_metrics.compute()\n",
    "        for metric_key, score in scores.items():\n",
    "            if isinstance(score, torch.Tensor):\n",
    "                scores[metric_key] = score.item()\n",
    "        scores.update({\"loss\": np.mean(self.train_losses)})\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def compute_valid(self) -> Dict[str, Any]:\n",
    "        scores = self.valid_metrics.compute()\n",
    "        for metric_key, score in scores.items():\n",
    "            if isinstance(score, torch.Tensor):\n",
    "                scores[metric_key] = score.item()\n",
    "        scores.update({\"loss\": np.mean(self.valid_losses)})\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def reset(self):\n",
    "        self.train_metrics.reset()\n",
    "        self.valid_metrics.reset()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "\n",
    "    def print_summary(self, epoch_idx: int):\n",
    "        train_result = self.compute_train()\n",
    "        valid_result = self.compute_valid()\n",
    "\n",
    "        assert list(train_result.keys()) == list(valid_result.keys())\n",
    "\n",
    "        pt = PrettyTable()\n",
    "        pt.field_names = [f\"epoch {epoch_idx}\"] + list(train_result.keys())\n",
    "\n",
    "        train_row = [\"train\"]\n",
    "        for score in train_result.values():\n",
    "            train_row.append(round(score, 3))\n",
    "        pt.add_row(train_row)\n",
    "\n",
    "        valid_row = [\"valid\"]\n",
    "        for score in valid_result.values():\n",
    "            valid_row.append(round(score, 3))\n",
    "        pt.add_row(valid_row)\n",
    "\n",
    "        print(pt, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56c06a9b-376e-4868-8edb-8f1b96f5b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model: AutoModelForAudioClassification,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    max_epoch: int = 64,\n",
    "    lr: float = 5e-4,\n",
    "    gradient_accumulate_step: int = 1,\n",
    "    early_stop_patience: int = 5,\n",
    ") -> dict:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    metric_scores = MetricScore().to(device)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        [{\"params\": module.parameters(), \"lr\": lr if name == \"classifier\" else lr * 0.1} for name, module in model.named_children()],\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    # run finetune\n",
    "    best_score = 0.0\n",
    "    best_state_dict = None\n",
    "\n",
    "    early_stop_count = 0\n",
    "    for epoch_idx in range(1, max_epoch):\n",
    "        with torch.set_grad_enabled(True), tqdm(total=len(train_loader), desc=f\"[Epoch {epoch_idx}/{max_epoch}] training\", leave=False) as pbar:\n",
    "            model.train()\n",
    "            for step_idx, batch in enumerate(train_loader, 1):\n",
    "                batch = batch.to(device)\n",
    "                \n",
    "                # print(batch['input_features'].shape)\n",
    "                output = model(**batch)\n",
    "                loss = output.loss\n",
    "                loss.backward()\n",
    "\n",
    "                if step_idx % gradient_accumulate_step == 0 or step_idx == len(train_loader):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()            \n",
    "\n",
    "                metric_scores.add_train_loss(loss)\n",
    "                metric_scores.add_train_metrics(output.logits, batch.labels)\n",
    "\n",
    "                pbar.update()\n",
    "                pbar.set_postfix({\"train loss\": loss.item()})\n",
    "\n",
    "        with torch.set_grad_enabled(False), tqdm(total=len(valid_loader), desc=f\"[Epoch {epoch_idx}/{max_epoch}] validation\", leave=False) as pbar:\n",
    "            model.eval()\n",
    "            for batch in valid_loader:\n",
    "                batch = batch.to(device)\n",
    "\n",
    "                output = model(**batch)\n",
    "                loss = output.loss\n",
    "\n",
    "                metric_scores.add_valid_loss(loss)\n",
    "                metric_scores.add_valid_metrics(output.logits, batch.labels)\n",
    "\n",
    "                pbar.update()\n",
    "                pbar.set_postfix({\"valid loss\": loss.item()})\n",
    "\n",
    "        epoch_score = metric_scores.compute_valid()[\"accuracy\"]\n",
    "        metric_scores.print_summary(epoch_idx=epoch_idx)\n",
    "        metric_scores.reset()\n",
    "\n",
    "        lr_scheduler.step(epoch_score)\n",
    "\n",
    "        if epoch_score > best_score:\n",
    "            best_score = epoch_score\n",
    "            best_state_dict = model.state_dict()\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count == early_stop_patience:\n",
    "                print(\"*** EARLY STOPPED ***\")\n",
    "                break\n",
    "    \n",
    "    return best_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00aa444c-32e7-4642-b6b4-fdeb9dfd3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(\n",
    "    model: AutoModelForAudioClassification,\n",
    "    feature_extractor: AutoFeatureExtractor,\n",
    "    test_dataset: Dataset,\n",
    "    batch_size: int = 16,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predict_logits = {\"id\": []}\n",
    "    predict_logits.update({class_id: [] for class_id in range(6)})\n",
    "\n",
    "    predict_class = {\"id\": [], \"label\": []}\n",
    "\n",
    "    for batch_idx in tqdm(range(0, len(test_dataset), batch_size), desc=\"prediction\"):\n",
    "        bs, bi = batch_idx, batch_idx + batch_size\n",
    "        batch = test_dataset[bs:bi]\n",
    "\n",
    "        if hasattr(feature_extractor, \"nb_max_frames\"):\n",
    "            padding = \"max_length\"\n",
    "        else:\n",
    "            padding = \"longest\"\n",
    "\n",
    "        model_inputs = feature_extractor(\n",
    "            batch[\"waveform\"],\n",
    "            sampling_rate=feature_extractor.sampling_rate,\n",
    "            padding=padding,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        model_output = model(**model_inputs)\n",
    "\n",
    "        batch_logits = model_output.logits.cpu()\n",
    "        batch_predict = model_output.logits.argmax(dim=-1).cpu()\n",
    "\n",
    "        predict_logits[\"id\"] += batch[\"id\"]\n",
    "        for class_id in range(6):\n",
    "            predict_logits[class_id] += batch_logits[:, class_id].tolist()\n",
    "\n",
    "        predict_class[\"id\"] += batch[\"id\"]\n",
    "        predict_class[\"label\"] += batch_predict.tolist()\n",
    "\n",
    "    predict_logits = pd.DataFrame(predict_logits)\n",
    "    predict_class = pd.DataFrame(predict_class)\n",
    "\n",
    "    return predict_logits, predict_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e974c7a9-b41d-48e7-b3fa-2d1639dc8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = audiomentations.OneOf(\n",
    "    [\n",
    "        audiomentations.AddGaussianNoise(p=0.75),\n",
    "        audiomentations.PitchShift(p=0.75),\n",
    "        audiomentations.PeakingFilter(p=0.75),\n",
    "        audiomentations.SevenBandParametricEQ(p=0.75),\n",
    "        audiomentations.BandPassFilter(p=0.75),\n",
    "        audiomentations.BandStopFilter(p=0.75),\n",
    "        audiomentations.AirAbsorption(p=0.75),\n",
    "        audiomentations.ClippingDistortion(p=0.75),\n",
    "        audiomentations.HighPassFilter(p=0.75),\n",
    "        audiomentations.HighShelfFilter(p=0.75),\n",
    "        audiomentations.Limiter(p=0.75),\n",
    "        audiomentations.LowPassFilter(p=0.75),\n",
    "        audiomentations.LowShelfFilter(p=0.75),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed398222-a85b-4bf7-ba2d-5d3cbe2217ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train data and valid data\n",
    "df = pd.read_csv(config.train_csv)\n",
    "df[\"path\"] = df[\"path\"].apply(lambda x: os.path.join(os.path.dirname(config.train_csv), x[2:]))\n",
    "\n",
    "if config.k_fold_num > 0:\n",
    "    skf = StratifiedKFold(n_splits=config.k_fold_num)\n",
    "    train_indices, valid_indices = list(skf.split(df, df[\"label\"]))[config.k_fold_idx]\n",
    "    train_df, valid_df = df.iloc[train_indices], df.iloc[valid_indices]\n",
    "else:\n",
    "    train_df, valid_df = train_test_split(df, train_size=0.9, stratify=df[\"label\"], random_state=seed)\n",
    "\n",
    "# create test data\n",
    "test_df = pd.read_csv(config.test_csv)\n",
    "test_df[\"path\"] = test_df[\"path\"].apply(lambda x: os.path.join(os.path.dirname(config.test_csv), x[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e99efdb3-a7d8-445f-a5c2-5872584849c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    feature_extractor=feature_extractor,\n",
    "    transforms=transforms,\n",
    ")\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     train_, \n",
    "#     # train_dataset,\n",
    "#     batch_size=config.batch_size,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=data_module.collate_fn,\n",
    "# )\n",
    "# valid_loader = DataLoader(\n",
    "#     ds,\n",
    "#     batch_size=config.batch_size,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=data_module.collate_fn,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07c8da3c-2b36-4eb9-a587-a9f17ecb19c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MyDataSet' object has no attribute '_indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulate_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulate_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stop_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(best_state_dict)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39msave_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, train_loader, valid_loader, max_epoch, lr, gradient_accumulate_step, early_stop_patience)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m), tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] training\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     30\u001b[0m         batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# print(batch['input_features'].shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py:2367\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2362\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterate through the examples.\u001b[39;00m\n\u001b[1;32m   2363\u001b[0m \n\u001b[1;32m   2364\u001b[0m \u001b[38;5;124;03m    If a formatting is set with :meth:`Dataset.set_format` rows will be returned with the\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m \u001b[38;5;124;03m    selected format.\u001b[39;00m\n\u001b[1;32m   2366\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2368\u001b[0m         \u001b[38;5;66;03m# Fast iteration\u001b[39;00m\n\u001b[1;32m   2369\u001b[0m         \u001b[38;5;66;03m# Benchmark: https://gist.github.com/mariosasko/0248288a2e3a7556873969717c1fe52b (fast_iter_batch)\u001b[39;00m\n\u001b[1;32m   2370\u001b[0m         format_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2371\u001b[0m         formatter \u001b[38;5;241m=\u001b[39m get_formatter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MyDataSet' object has no attribute '_indices'"
     ]
    }
   ],
   "source": [
    "best_state_dict = fit(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    max_epoch=config.max_epoch,\n",
    "    lr=config.lr,\n",
    "    gradient_accumulate_step=config.gradient_accumulate_step,\n",
    "    early_stop_patience=config.early_stop_patience,\n",
    ")\n",
    "\n",
    "model.load_state_dict(best_state_dict)\n",
    "model.save_pretrained(os.path.join(config.save_dir, \"best_model\"))\n",
    "feature_extractor.save_pretrained(os.path.join(config.save_dir, \"best_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498783f2-b8fd-4925-9471-9d149eb4d54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2953d09-678d-4b7e-bb67-27eed28bd061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3860fba-c4d7-4b39-931d-ad7e3d92d014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98058c68-f3eb-43e4-bfd4-5febbc1637cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a30b2-6e78-4fed-8a22-3d4976d2e460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef0d75-c50c-413c-87b5-3bafa29b576a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78d477-de8b-496a-a57a-46370eb5fc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk8574_3.10 [~/.conda/envs/mk8574_3.10/]",
   "language": "python",
   "name": "conda_mk8574_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
