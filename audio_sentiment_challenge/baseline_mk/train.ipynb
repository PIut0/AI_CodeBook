{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9081c53e-7e99-4909-9007-919f7072be20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from dataclasses import asdict, dataclass\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "from datetime import datetime\n",
    "import audiomentations\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "from transformers.feature_extraction_utils import BatchFeature\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d31d224-cb01-4d9d-84c5-8b980bfd53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # data args\n",
    "    train_csv: str = \"/scratch/network/mk8574/audio_sentiment_challenge/data/train.csv\"\n",
    "    test_csv: str = \"/scratch/network/mk8574/audio_sentiment_challenge/data/test.csv\"\n",
    "\n",
    "    # model args\n",
    "    pretrained_name: str = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "    \n",
    "    train_serial = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"|\" + pretrained_name.replace(\"/\", \"|\")\n",
    "\n",
    "    # k-fold\n",
    "    k_fold_num: int = 0  # if you want to use k-fold validation, set positive integer value.\n",
    "    k_fold_idx: int = 1\n",
    "\n",
    "    # save dir\n",
    "    save_dir: str = f\"/scratch/network/mk8574/audio_sentiment_challenge/baseline_mk/results/{train_serial}/\"\n",
    "\n",
    "    # hparams\n",
    "    seed: int = 42\n",
    "    lr: float = 5e-4\n",
    "    batch_size: int = 10\n",
    "    gradient_accumulate_step: int = 4  # total batch size = batch_size * gradient_accumulate_step\n",
    "    max_epoch: int = batch_size * gradient_accumulate_step\n",
    "    early_stop_patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137033d4-dfe2-4ec8-bf71-e0edaeb0a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "if not os.path.exists(config.save_dir):\n",
    "    os.makedirs(config.save_dir)\n",
    "\n",
    "with open(os.path.join(config.save_dir, \"config.json\"), \"w\") as config_file:\n",
    "    json.dump(asdict(config), config_file, indent=4, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb4a6b5-7304-4e34-905e-e05029bf27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = config.seed\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9e314c-eb16-468e-a766-908835546e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained(pretrained_name: str) -> Tuple[AutoModelForAudioClassification, AutoFeatureExtractor]:\n",
    "    # model\n",
    "    model = AutoModelForAudioClassification.from_pretrained(pretrained_name)\n",
    "\n",
    "    model.config.num_labels = 6\n",
    "    model.classifier = nn.Linear(in_features=model.projector.out_features, out_features=6)\n",
    "    nn.init.kaiming_normal_(model.classifier.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "    nn.init.zeros_(model.classifier.bias)\n",
    "\n",
    "    # feature extractor\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(pretrained_name)\n",
    "\n",
    "    return model, feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c774542e-bd36-472e-8806-52ae0f80b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 5002/5002 [00:00<00:00, 43392.19it/s]\n",
      "Resolving data files: 100%|██████████| 1882/1882 [00:00<00:00, 526561.28it/s]\n",
      "Downloading data files: 100%|██████████| 5001/5001 [00:00<00:00, 120796.53it/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 18396.07it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 471.38it/s]\n",
      "Downloading data files: 100%|██████████| 1881/1881 [00:00<00:00, 110387.23it/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 20867.18it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 489.59it/s]\n",
      "Generating train split: 5001 examples [00:00, 27506.82 examples/s]\n",
      "Generating test split: 1881 examples [00:00, 16274.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"audiofolder\", data_dir = '../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7482947c-706b-4498-bf5d-d24321311732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor: AutoFeatureExtractor,\n",
    "        transforms: list = None,\n",
    "    ):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def to_dataset(self, df: pd.DataFrame) -> Dataset:\n",
    "        def load_waveform(row):\n",
    "            waveform, sample_rate = librosa.load(row[\"path\"])\n",
    "            waveform = librosa.resample(\n",
    "                waveform,\n",
    "                orig_sr=sample_rate,\n",
    "                target_sr=self.feature_extractor.sampling_rate,\n",
    "            )\n",
    "            row[\"waveform\"] = waveform\n",
    "\n",
    "            return row\n",
    "\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        dataset = dataset.map(load_waveform, num_proc=4)\n",
    "        if \"label\" in dataset.column_names:\n",
    "            dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def apply_transforms(self, dataset: Dataset) -> Dataset:\n",
    "        def apply_transforms(batch):\n",
    "            waveforms = [self.transforms(samples=np.array(waveform, dtype=np.float32), sample_rate=self.feature_extractor.sampling_rate) for waveform in batch[\"waveform\"]]\n",
    "            batch[\"waveform\"] = waveforms\n",
    "\n",
    "            return batch\n",
    "\n",
    "        if self.transforms:\n",
    "            dataset = dataset.with_transform(apply_transforms)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def collate_fn(self, batch: list) -> BatchFeature:\n",
    "        if hasattr(self.feature_extractor, \"nb_max_frames\"):\n",
    "            padding = \"max_length\"\n",
    "        else:\n",
    "            padding = \"longest\"\n",
    "\n",
    "        waveforms = [data[\"waveform\"] for data in batch]\n",
    "        model_inputs = self.feature_extractor(\n",
    "            waveforms,\n",
    "            sampling_rate=self.feature_extractor.sampling_rate,\n",
    "            padding=padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if \"labels\" in batch[0]:\n",
    "            labels = [data[\"labels\"] for data in batch]\n",
    "            model_inputs[\"labels\"] = torch.tensor(labels)\n",
    "\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906a4036-5651-4944-9b3b-75c7b2c761bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricScore(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        metrics = MetricCollection(\n",
    "            {\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=6),\n",
    "                \"recall\": MulticlassRecall(num_classes=6),\n",
    "                \"precision\": MulticlassPrecision(num_classes=6),\n",
    "                \"f1\": MulticlassF1Score(num_classes=6),\n",
    "            }\n",
    "        )\n",
    "        self.train_metrics = metrics.clone()\n",
    "        self.valid_metrics = metrics.clone()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "\n",
    "    def add_train_metrics(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        self.train_metrics.update(logits, labels)\n",
    "\n",
    "    def add_valid_metrics(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        self.valid_metrics.update(logits, labels)\n",
    "\n",
    "    def add_train_loss(self, loss: torch.Tensor):\n",
    "        self.train_losses.append(loss.item())\n",
    "\n",
    "    def add_valid_loss(self, loss: torch.Tensor):\n",
    "        self.valid_losses.append(loss.item())\n",
    "\n",
    "    def compute_train(self) -> Dict[str, Any]:\n",
    "        scores = self.train_metrics.compute()\n",
    "        for metric_key, score in scores.items():\n",
    "            if isinstance(score, torch.Tensor):\n",
    "                scores[metric_key] = score.item()\n",
    "        scores.update({\"loss\": np.mean(self.train_losses)})\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def compute_valid(self) -> Dict[str, Any]:\n",
    "        scores = self.valid_metrics.compute()\n",
    "        for metric_key, score in scores.items():\n",
    "            if isinstance(score, torch.Tensor):\n",
    "                scores[metric_key] = score.item()\n",
    "        scores.update({\"loss\": np.mean(self.valid_losses)})\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def reset(self):\n",
    "        self.train_metrics.reset()\n",
    "        self.valid_metrics.reset()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "\n",
    "    def print_summary(self, epoch_idx: int):\n",
    "        train_result = self.compute_train()\n",
    "        valid_result = self.compute_valid()\n",
    "\n",
    "        assert list(train_result.keys()) == list(valid_result.keys())\n",
    "\n",
    "        pt = PrettyTable()\n",
    "        pt.field_names = [f\"epoch {epoch_idx}\"] + list(train_result.keys())\n",
    "\n",
    "        train_row = [\"train\"]\n",
    "        for score in train_result.values():\n",
    "            train_row.append(round(score, 3))\n",
    "        pt.add_row(train_row)\n",
    "\n",
    "        valid_row = [\"valid\"]\n",
    "        for score in valid_result.values():\n",
    "            valid_row.append(round(score, 3))\n",
    "        pt.add_row(valid_row)\n",
    "\n",
    "        print(pt, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26b84ed-7e35-470f-bb8e-2edbe3cc2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model: AutoModelForAudioClassification,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    max_epoch: int = 64,\n",
    "    lr: float = 5e-4,\n",
    "    gradient_accumulate_step: int = 1,\n",
    "    early_stop_patience: int = 5,\n",
    ") -> dict:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    metric_scores = MetricScore().to(device)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        [{\"params\": module.parameters(), \"lr\": lr if name == \"classifier\" else lr * 0.1} for name, module in model.named_children()],\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    # run finetune\n",
    "    best_score = 0.0\n",
    "    best_state_dict = None\n",
    "\n",
    "    early_stop_count = 0\n",
    "    for epoch_idx in range(1, max_epoch):\n",
    "        with torch.set_grad_enabled(True), tqdm(total=len(train_loader), desc=f\"[Epoch {epoch_idx}/{max_epoch}] training\", leave=False) as pbar:\n",
    "            model.train()\n",
    "            for step_idx, batch in enumerate(train_loader, 1):\n",
    "                batch = batch.to(device)\n",
    "\n",
    "                output = model(**batch)\n",
    "                loss = output.loss\n",
    "                loss.backward()\n",
    "\n",
    "                if step_idx % gradient_accumulate_step == 0 or step_idx == len(train_loader):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()            \n",
    "\n",
    "                metric_scores.add_train_loss(loss)\n",
    "                metric_scores.add_train_metrics(output.logits, batch.labels)\n",
    "\n",
    "                pbar.update()\n",
    "                pbar.set_postfix({\"train loss\": loss.item()})\n",
    "\n",
    "        with torch.set_grad_enabled(False), tqdm(total=len(valid_loader), desc=f\"[Epoch {epoch_idx}/{max_epoch}] validation\", leave=False) as pbar:\n",
    "            model.eval()\n",
    "            for batch in valid_loader:\n",
    "                batch = batch.to(device)\n",
    "\n",
    "                output = model(**batch)\n",
    "                loss = output.loss\n",
    "\n",
    "                metric_scores.add_valid_loss(loss)\n",
    "                metric_scores.add_valid_metrics(output.logits, batch.labels)\n",
    "\n",
    "                pbar.update()\n",
    "                pbar.set_postfix({\"valid loss\": loss.item()})\n",
    "\n",
    "        epoch_score = metric_scores.compute_valid()[\"accuracy\"]\n",
    "        metric_scores.print_summary(epoch_idx=epoch_idx)\n",
    "        metric_scores.reset()\n",
    "\n",
    "        lr_scheduler.step(epoch_score)\n",
    "\n",
    "        if epoch_score > best_score:\n",
    "            best_score = epoch_score\n",
    "            best_state_dict = model.state_dict()\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count == early_stop_patience:\n",
    "                print(\"*** EARLY STOPPED ***\")\n",
    "                break\n",
    "    \n",
    "    return best_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "935953e2-1a60-4fec-b597-f039d23f4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(\n",
    "    model: AutoModelForAudioClassification,\n",
    "    feature_extractor: AutoFeatureExtractor,\n",
    "    test_dataset: Dataset,\n",
    "    batch_size: int = 16,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predict_logits = {\"id\": []}\n",
    "    predict_logits.update({class_id: [] for class_id in range(6)})\n",
    "\n",
    "    predict_class = {\"id\": [], \"label\": []}\n",
    "\n",
    "    for batch_idx in tqdm(range(0, len(test_dataset), batch_size), desc=\"prediction\"):\n",
    "        bs, bi = batch_idx, batch_idx + batch_size\n",
    "        batch = test_dataset[bs:bi]\n",
    "\n",
    "        if hasattr(feature_extractor, \"nb_max_frames\"):\n",
    "            padding = \"max_length\"\n",
    "        else:\n",
    "            padding = \"longest\"\n",
    "\n",
    "        model_inputs = feature_extractor(\n",
    "            batch[\"waveform\"],\n",
    "            sampling_rate=feature_extractor.sampling_rate,\n",
    "            padding=padding,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        model_output = model(**model_inputs)\n",
    "\n",
    "        batch_logits = model_output.logits.cpu()\n",
    "        batch_predict = model_output.logits.argmax(dim=-1).cpu()\n",
    "\n",
    "        predict_logits[\"id\"] += batch[\"id\"]\n",
    "        for class_id in range(6):\n",
    "            predict_logits[class_id] += batch_logits[:, class_id].tolist()\n",
    "\n",
    "        predict_class[\"id\"] += batch[\"id\"]\n",
    "        predict_class[\"label\"] += batch_predict.tolist()\n",
    "\n",
    "    predict_logits = pd.DataFrame(predict_logits)\n",
    "    predict_class = pd.DataFrame(predict_class)\n",
    "\n",
    "    return predict_logits, predict_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd815c9-fa66-47b7-8a13-9da442eb54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = audiomentations.OneOf(\n",
    "    [\n",
    "        audiomentations.AddGaussianNoise(p=0.75),\n",
    "        audiomentations.PitchShift(p=0.75),\n",
    "        audiomentations.PeakingFilter(p=0.75),\n",
    "        audiomentations.SevenBandParametricEQ(p=0.75),\n",
    "        audiomentations.BandPassFilter(p=0.75),\n",
    "        audiomentations.BandStopFilter(p=0.75),\n",
    "        audiomentations.AirAbsorption(p=0.75),\n",
    "        audiomentations.ClippingDistortion(p=0.75),\n",
    "        audiomentations.HighPassFilter(p=0.75),\n",
    "        audiomentations.HighShelfFilter(p=0.75),\n",
    "        audiomentations.Limiter(p=0.75),\n",
    "        audiomentations.LowPassFilter(p=0.75),\n",
    "        audiomentations.LowShelfFilter(p=0.75),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62313ec3-49f3-4d78-84a6-105a8b81adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, feature_extractor = load_pretrained(config.pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c763b4a7-acc9-4f58-bb80-d77dfc123188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train data and valid data\n",
    "df = pd.read_csv(config.train_csv)\n",
    "df[\"path\"] = df[\"path\"].apply(lambda x: os.path.join(os.path.dirname(config.train_csv), x[2:]))\n",
    "\n",
    "# create test data\n",
    "test_df = pd.read_csv(config.test_csv)\n",
    "test_df[\"path\"] = test_df[\"path\"].apply(lambda x: os.path.join(os.path.dirname(config.test_csv), x[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a74c4a8e-5314-4a66-bec6-e391656bcc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train data and valid data\n",
    "df = pd.read_csv(config.train_csv)\n",
    "df[\"path\"] = df[\"path\"].apply(lambda x: os.path.join(os.path.dirname(config.train_csv), x[2:]))\n",
    "\n",
    "if config.k_fold_num > 0:\n",
    "    skf = StratifiedKFold(n_splits=config.k_fold_num)\n",
    "    train_indices, valid_indices = list(skf.split(df, df[\"label\"]))[config.k_fold_idx]\n",
    "    train_df, valid_df = df.iloc[train_indices], df.iloc[valid_indices]\n",
    "else:\n",
    "    train_df, valid_df = train_test_split(df, train_size=0.9, stratify=df[\"label\"], random_state=seed)\n",
    "\n",
    "# create test data\n",
    "test_df = pd.read_csv(config.test_csv)\n",
    "test_df[\"path\"] = test_df[\"path\"].apply(lambda x: os.path.join(os.path.dirname(config.test_csv), x[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20358084-813d-4a50-9a3e-ca9880ae05e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 4500/4500 [00:06<00:00, 717.50 examples/s] \n",
      "Map (num_proc=4): 100%|██████████| 501/501 [00:01<00:00, 299.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = DataModule(\n",
    "    feature_extractor=feature_extractor,\n",
    "    transforms=transforms,\n",
    ")\n",
    "train_dataset = data_module.apply_transforms(data_module.to_dataset(train_df))\n",
    "valid_dataset = data_module.to_dataset(valid_df)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_module.collate_fn,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_module.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ade73d61-32f1-4452-8859-f11347b5e5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----------+--------+-------+\n",
      "| epoch 1 | accuracy |   f1  | precision | recall |  loss |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "|  train  |  0.439   | 0.431 |   0.444   | 0.439  | 1.398 |\n",
      "|  valid  |  0.582   | 0.567 |   0.597   | 0.582  | 1.008 |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----------+--------+-------+\n",
      "| epoch 2 | accuracy |   f1  | precision | recall |  loss |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "|  train  |  0.643   |  0.64 |    0.64   | 0.643  | 0.949 |\n",
      "|  valid  |  0.695   | 0.682 |   0.706   | 0.695  | 0.794 |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----------+--------+-------+\n",
      "| epoch 3 | accuracy |   f1  | precision | recall |  loss |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "|  train  |   0.71   | 0.709 |   0.709   |  0.71  | 0.783 |\n",
      "|  valid  |  0.771   | 0.768 |    0.77   | 0.771  | 0.612 |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----------+--------+-------+\n",
      "| epoch 4 | accuracy |   f1  | precision | recall |  loss |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "|  train  |  0.747   | 0.745 |   0.745   | 0.747  | 0.683 |\n",
      "|  valid  |  0.727   | 0.725 |   0.745   | 0.727  | 0.734 |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----------+--------+-------+\n",
      "| epoch 5 | accuracy |   f1  | precision | recall |  loss |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "|  train  |  0.767   | 0.766 |   0.765   | 0.767  |  0.63 |\n",
      "|  valid  |  0.751   |  0.75 |   0.753   | 0.751  | 0.679 |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----------+--------+-------+\n",
      "| epoch 6 | accuracy |   f1  | precision | recall |  loss |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "|  train  |  0.794   | 0.793 |   0.793   | 0.794  | 0.559 |\n",
      "|  valid  |  0.766   | 0.762 |   0.766   | 0.766  | 0.683 |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----------+--------+-------+\n",
      "| epoch 7 | accuracy |   f1  | precision | recall |  loss |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "|  train  |  0.815   | 0.814 |   0.814   | 0.815  | 0.515 |\n",
      "|  valid  |  0.778   | 0.779 |   0.784   | 0.778  | 0.639 |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----------+--------+-------+\n",
      "| epoch 8 | accuracy |   f1  | precision | recall |  loss |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "|  train  |  0.826   | 0.826 |   0.827   | 0.826  | 0.476 |\n",
      "|  valid  |  0.782   | 0.785 |   0.796   | 0.782  | 0.689 |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----------+--------+-------+\n",
      "| epoch 9 | accuracy |   f1  | precision | recall |  loss |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "|  train  |  0.836   | 0.836 |   0.837   | 0.836  | 0.436 |\n",
      "|  valid  |  0.766   |  0.77 |   0.797   | 0.766  | 0.747 |\n",
      "+---------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-----------+--------+-------+\n",
      "| epoch 10 | accuracy |   f1  | precision | recall |  loss |\n",
      "+----------+----------+-------+-----------+--------+-------+\n",
      "|  train   |  0.856   | 0.856 |   0.856   | 0.856  | 0.407 |\n",
      "|  valid   |  0.785   | 0.784 |   0.786   | 0.785  | 0.657 |\n",
      "+----------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-----------+--------+-------+\n",
      "| epoch 11 | accuracy |   f1  | precision | recall |  loss |\n",
      "+----------+----------+-------+-----------+--------+-------+\n",
      "|  train   |  0.867   | 0.867 |   0.867   | 0.867  | 0.374 |\n",
      "|  valid   |  0.761   | 0.764 |   0.776   | 0.761  |  0.89 |\n",
      "+----------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-----------+--------+-------+\n",
      "| epoch 12 | accuracy |   f1  | precision | recall |  loss |\n",
      "+----------+----------+-------+-----------+--------+-------+\n",
      "|  train   |  0.867   | 0.867 |   0.868   | 0.867  | 0.353 |\n",
      "|  valid   |  0.777   | 0.776 |    0.78   | 0.777  | 0.733 |\n",
      "+----------+----------+-------+-----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulate_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulate_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stop_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(best_state_dict)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39msave_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, train_loader, valid_loader, max_epoch, lr, gradient_accumulate_step, early_stop_patience)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     30\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 32\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     34\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2115\u001b[0m, in \u001b[0;36mWav2Vec2ForSequenceClassification.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2115\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_feature_vector_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m     hidden_states[\u001b[38;5;241m~\u001b[39mpadding_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m   2117\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m padding_mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1158\u001b[0m, in \u001b[0;36mWav2Vec2PreTrainedModel._get_feature_vector_attention_mask\u001b[0;34m(self, feature_vector_length, attention_mask, add_adapter)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# these two operations makes sure that all values before the output lengths idxs are attended to\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m attention_mask[(torch\u001b[38;5;241m.\u001b[39marange(attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mdevice), output_lengths \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1158\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcumsum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflip([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mbool()\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attention_mask\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_state_dict = fit(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    max_epoch=config.max_epoch,\n",
    "    lr=config.lr,\n",
    "    gradient_accumulate_step=config.gradient_accumulate_step,\n",
    "    early_stop_patience=config.early_stop_patience,\n",
    ")\n",
    "\n",
    "model.load_state_dict(best_state_dict)\n",
    "model.save_pretrained(os.path.join(config.save_dir, \"best_model\"))\n",
    "feature_extractor.save_pretrained(os.path.join(config.save_dir, \"best_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c67ac4-3904-4bd6-9224-e5e749c38f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = data_module.to_dataset(test_df)\n",
    "predict_logits, predict_class = predict(model, feature_extractor, test_dataset, config.batch_size)\n",
    "\n",
    "predict_logits.to_csv(os.path.join(config.save_dir, \"predict_logits.csv\"), index=False)\n",
    "predict_class.to_csv(os.path.join(config.save_dir, \"predict_class.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk8574_3.10 [~/.conda/envs/mk8574_3.10/]",
   "language": "python",
   "name": "conda_mk8574_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
