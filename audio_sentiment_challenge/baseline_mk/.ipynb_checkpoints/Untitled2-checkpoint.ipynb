{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392efc7c-50f3-46f6-aad6-6e82eb3cb45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from dataclasses import asdict, dataclass\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import librosa\n",
    "import audiomentations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    device: int = 0\n",
    "    # data args\n",
    "    data_path: str = \"/scratch/network/mk8574/audio_sentiment_challenge/data\"\n",
    "    val_size: float = 0.2\n",
    "    \n",
    "    train_transforms: audiomentations.OneOf([\n",
    "        audiomentations.AddGaussianNoise(p=0)\n",
    "    ]) = None\n",
    "    # [\n",
    "    #     audiomentations.AddGaussianNoise(p=0.75),\n",
    "    #     audiomentations.PitchShift(p=0.75),\n",
    "    #     audiomentations.PeakingFilter(p=0.75),\n",
    "    #     audiomentations.SevenBandParametricEQ(p=0.75),\n",
    "    #     audiomentations.BandPassFilter(p=0.75),\n",
    "    #     audiomentations.BandStopFilter(p=0.75),\n",
    "    #     audiomentations.AirAbsorption(p=0.75),\n",
    "    #     audiomentations.ClippingDistortion(p=0.75),\n",
    "    #     audiomentations.HighPassFilter(p=0.75),\n",
    "    #     audiomentations.HighShelfFilter(p=0.75),\n",
    "    #     audiomentations.Limiter(p=0.75),\n",
    "    #     audiomentations.LowPassFilter(p=0.75),\n",
    "    #     audiomentations.LowShelfFilter(p=0.75),\n",
    "    # ]) = None\n",
    "\n",
    "    # save dir\n",
    "    train_serial = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir: str = f\"/scratch/network/mk8574/audio_sentiment_challenge/baseline_dy/results/{train_serial}\"\n",
    "\n",
    "    # model args\n",
    "    pretrained_name: str = \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
    "\n",
    "    # hparams\n",
    "    seed: int = 42\n",
    "    max_epoch: int = 32\n",
    "    lr: float = 5e-4\n",
    "    batch_size: int = 32\n",
    "    total_batch_size: int = 32\n",
    "    gradient_accumulate_step: int = 1  # total batch size = batch_size * gradient_accumulate_step\n",
    "    early_stop_patience = 5\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "device = torch.device(f'cuda:{config.device}') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(config.seed) # Seed 고정\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(config.data_path, 'train.csv'))\n",
    "train_df, valid_df = train_test_split(train_df, test_size=config.val_size, random_state=config.seed)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, df, feature_extractor, mode='train', transforms=None):\n",
    "        self.df = df\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(config.data_path, self.df['path'][idx][2:])\n",
    "        \n",
    "        waveform, sample_rate = librosa.load(path)\n",
    "        sr = self.feature_extractor.sampling_rate\n",
    "        waveform = librosa.resample(waveform, orig_sr=sample_rate, target_sr=sr)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            waveform = self.transforms(samples=np.array(waveform, dtype=np.float32), sample_rate=sr)\n",
    "        \n",
    "        input_values = self.feature_extractor(waveform, sampling_rate=sr, return_tensors=\"pt\", padding=True).input_values\n",
    "        \n",
    "        if self.mode!= 'test':\n",
    "            label = self.df['label'][idx]\n",
    "            return input_values.squeeze(), label\n",
    "        else:\n",
    "            return input_values.squeeze()\n",
    "\n",
    "        \n",
    "def collate_fn_yes_label(batch):\n",
    "    waveforms, labels = zip(*batch)\n",
    "    waveforms = pad_sequence([torch.tensor(wave) for wave in waveforms], batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    return waveforms, labels\n",
    "\n",
    "def collate_fn_no_label(batch):\n",
    "    waveforms = zip(*batch)\n",
    "    waveforms = pad_sequence([torch.tensor(wave) for wave in waveforms], batch_first=True)\n",
    "    return waveforms\n",
    "\n",
    "\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(config.pretrained_name)\n",
    "\n",
    "train_dataset = MyDataSet(train_df, feature_extractor, mode='train', transforms=config.train_transforms)\n",
    "valid_dataset = MyDataSet(valid_df, feature_extractor, mode='val')\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=config.batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn_yes_label,\n",
    "                          num_workers=16)\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                          batch_size=config.batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn_yes_label,\n",
    "                          num_workers=16)\n",
    "\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_name: str):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = AutoModelForAudioClassification.from_pretrained(pretrained_name)\n",
    "        \n",
    "        self.model.classifier = nn.Linear(in_features=self.model.projector.out_features, out_features=6)\n",
    "        nn.init.kaiming_normal_(self.model.classifier.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        nn.init.zeros_(self.model.classifier.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output.logits\n",
    "    \n",
    "def validation(model, valid_loader, creterion):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for waveforms, labels in tqdm(iter(valid_loader)):\n",
    "            waveforms = waveforms.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(waveforms)            \n",
    "            loss = creterion(output, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).cpu().sum()\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    avg_loss = np.mean(val_loss)\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, valid_loader, optimizer, scheduler, mode = 'none'):\n",
    "    accumulation_step = int(config.total_batch_size / config.batch_size)\n",
    "    model.to(device)\n",
    "    creterion = nn.CrossEntropyLoss(label_smoothing = 0.0).to(device)\n",
    "    #creterion = nn.CrossEntropyLoss(label_smoothing = 0.0).to(device)\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(1, config.max_epoch+1):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        \n",
    "        for i, (waveforms, labels) in enumerate(tqdm(train_loader)):\n",
    "            waveforms = waveforms.to(device)\n",
    "            labels = labels.flatten().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(waveforms)\n",
    "            loss = creterion(output, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            if (i+1) % accumulation_step == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        avg_loss = np.mean(train_loss)\n",
    "        valid_loss, valid_acc = validation(model, valid_loader, creterion)\n",
    "        \n",
    "        return model, train_loader, valid_loader\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_model = model\n",
    "\n",
    "        print(f'epoch:[{epoch}] train loss:[{avg_loss:.5f}] valid_loss:[{valid_loss:.5f}] valid_acc:[{valid_acc:.5f}]')\n",
    "    \n",
    "    print(f'best_acc:{best_acc:.5f}')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "model = MyModel(config.pretrained_name)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [{\"params\": module.parameters(), \"lr\": config.lr if name == \"classifier\" else config.lr * 0.1} for name, module in model.named_children()],\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "best_model = train(model, train_loader, valid_loader, optimizer, scheduler)\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(config.data_path, 'test.csv'))\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_dataset = MyDataSet(test_df, feature_extractor, mode='test')\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                        batch_size=config.batch_size,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate_fn_no_label,\n",
    "                        num_workers=16)\n",
    "\n",
    "def inference(model, test_loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x in tqdm(iter(test_loader)):\n",
    "            x = x.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "\n",
    "            preds += output.argmax(-1).detach().cpu().numpy().tolist()\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "# preds = inference(best_model, test_loader)\n",
    "\n",
    "# submission = pd.read_csv(os.path.join(config.data_path, 'sample_submission.csv'))\n",
    "# submission['label'] = preds\n",
    "# submission.to_csv(os.path.join(config.save_dir, 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6599fb0-8c38-4933-9a5c-aa952bd7cb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f325072c7c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "642aadb8-6d1a-470e-a360-c2d9e133524b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 976.00 MiB. GPU 0 has a total capacty of 39.56 GiB of which 14.81 MiB is free. Including non-PyTorch memory, this process has 39.54 GiB memory in use. Of the allocated memory 37.95 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (waveforms, labels) \u001b[38;5;129;01min\u001b[39;00m tqdm((train_loader)):\n\u001b[1;32m      4\u001b[0m     waveforms \u001b[38;5;241m=\u001b[39m waveforms\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(waveforms\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 158\u001b[0m, in \u001b[0;36mMyModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 158\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2095\u001b[0m, in \u001b[0;36mWav2Vec2ForSequenceClassification.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   2092\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   2093\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_weighted_layer_sum \u001b[38;5;28;01melse\u001b[39;00m output_hidden_states\n\u001b[0;32m-> 2095\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwav2vec2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_weighted_layer_sum:\n\u001b[1;32m   2104\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs[_HIDDEN_STATES_START_POSITION]\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1547\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1542\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1543\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1544\u001b[0m )\n\u001b[1;32m   1545\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1547\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1548\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m extract_features\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;66;03m# compute reduced attention_mask corresponding to feature vectors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:459\u001b[0m, in \u001b[0;36mWav2Vec2FeatureEncoder.forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m    454\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    455\u001b[0m             conv_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    456\u001b[0m             hidden_states,\n\u001b[1;32m    457\u001b[0m         )\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:334\u001b[0m, in \u001b[0;36mWav2Vec2LayerNormConvLayer.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 334\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    337\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 976.00 MiB. GPU 0 has a total capacty of 39.56 GiB of which 14.81 MiB is free. Including non-PyTorch memory, this process has 39.54 GiB memory in use. Of the allocated memory 37.95 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "for (waveforms, labels) in tqdm((train_loader)):\n",
    "    waveforms = waveforms.to(device)\n",
    "    \n",
    "    output = best_model[0](waveforms)\n",
    "    preds += output.argmax(-1).detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e033fa-f772-490a-939b-c8e075acd27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_avg(PATH):\n",
    "    # path = '../data/train/TRAIN_4677.wav' \n",
    "    # os.path.join(config.data_path, self.df['path'][idx][2:])\n",
    "    waveform, sample_rate = librosa.load(PATH)\n",
    "    waveform = waveform ** 2\n",
    "\n",
    "    mean_avg = sum(waveform)/len(waveform)\n",
    "    \n",
    "    return mean_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b6160a-6174-4047-b08a-212df158c4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    709\n",
       "1    691\n",
       "3    683\n",
       "2    683\n",
       "5    678\n",
       "4    556\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb85491-2470-4929-92fc-879e72aa239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lst = []\n",
    "for i in range(6):\n",
    "    lst.append([])\n",
    "    for PATH in train_df[train_df['label'] == i]['path']:\n",
    "        lst[i].append(mean_avg(os.path.join('../data', PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ef26f0-f62b-42bc-9279-a94f638c8236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 class: mean 12.024333908121214 std 15.510942752760448\n",
      "1 class: mean 2.972858633807044 std 7.135884436007252\n",
      "2 class: mean 0.2498147168579368 std 0.7718024815051593\n",
      "3 class: mean 1.1357541808218523 std 2.6516332104686273\n",
      "4 class: mean 0.4709839987152159 std 0.4534721173141672\n",
      "5 class: mean 3.180424512171136 std 6.207471477453297\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "for i in range(6):\n",
    "    plt.hist(lst[i], label = i, bins = np.arange(0, 0.02, 0.0003))\n",
    "    \n",
    "    print(f\"{i} class: mean {sum(lst[i])/len(lst[i]) * 1000} std {statistics.pstdev(lst[i]) * 1000}\")\n",
    "#     plt.show()\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b0237b-a855-4006-93e3-332e647b03f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xg_clf = XGBClassifier(n_estmiators = 2, max_depth = 3, learning_rate = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576e3070-f89b-4dd6-b75e-1490e10ea625",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xg_clf\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, Y_train)\n\u001b[1;32m      3\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m xg_clf\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "xg_clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = xg_clf.predict(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ff5a3-db03-4496-b042-08605d8f2d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk8574_3.10 [~/.conda/envs/mk8574_3.10/]",
   "language": "python",
   "name": "conda_mk8574_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
