{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab1932a-964f-42a4-8a1b-dd232d360bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from dataclasses import asdict, dataclass\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "from datetime import datetime\n",
    "import audiomentations\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, AutoTokenizer\n",
    "from transformers.feature_extraction_utils import BatchFeature\n",
    "from transformers import AutoFeatureExtractor, WhisperForAudioClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234cfdc5-bbfa-40bc-8ff6-43f1071e7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # data args\n",
    "    train_csv: str = \"/scratch/network/mk8574/audio_sentiment_challenge/data/train.csv\"\n",
    "    test_csv: str = \"/scratch/network/mk8574/audio_sentiment_challenge/data/test.csv\"\n",
    "\n",
    "    # model args\n",
    "    pretrained_name: str = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "    \n",
    "    train_serial = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"|\" + pretrained_name.replace(\"/\", \"|\")\n",
    "\n",
    "    # k-fold\n",
    "    k_fold_num: int = 0  # if you want to use k-fold validation, set positive integer value.\n",
    "    k_fold_idx: int = 1\n",
    "\n",
    "    # save dir\n",
    "    save_dir: str = f\"/scratch/network/mk8574/audio_sentiment_challenge/baseline_mk/results/{train_serial}/\"\n",
    "\n",
    "    # hparams\n",
    "    seed: int = 42\n",
    "    lr: float = 5e-4\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulate_step: int = 4  # total batch size = batch_size * gradient_accumulate_step\n",
    "    max_epoch: int = batch_size * gradient_accumulate_step\n",
    "    early_stop_patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6fe578-f53f-49d7-94ef-12fe48050860",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "if not os.path.exists(config.save_dir):\n",
    "    os.makedirs(config.save_dir)\n",
    "\n",
    "with open(os.path.join(config.save_dir, \"config.json\"), \"w\") as config_file:\n",
    "    json.dump(asdict(config), config_file, indent=4, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73a817c-f6b2-49ea-8f4a-eafc0ac86524",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")\n",
    "model = WhisperForAudioClassification.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd00d23-78ee-49ea-87e4-a61180100f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = config.seed\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7923ed56-0586-44bf-b470-f881b509e179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 5002/5002 [00:00<00:00, 504421.73it/s]\n",
      "Resolving data files: 100%|██████████| 1882/1882 [00:00<00:00, 243602.03it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"audiofolder\", data_dir = '../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60960537-4261-4086-b704-f04526fad8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'label'],\n",
       "    num_rows: 5001\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ed39c6-68fe-409f-aad7-dfe042c5fef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/scratch/network/mk8574/audio_sentiment_challenge/data/train/TRAIN_0000.wav', 'array': array([0.00750732, 0.00820923, 0.00793457, ..., 0.        , 0.        ,\n",
      "       0.        ]), 'sampling_rate': 16000}, 'label': None}\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(ds['train']))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "098c4fac-b6ad-46cd-9a9e-6039938ec4e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/models/whisper/feature_extraction_whisper.py:216\u001b[0m, in \u001b[0;36mWhisperFeatureExtractor.__call__\u001b[0;34m(self, raw_speech, truncation, pad_to_multiple_of, return_tensors, return_attention_mask, padding, max_length, sampling_rate, do_normalize, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray([speech], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;28;01mfor\u001b[39;00m speech \u001b[38;5;129;01min\u001b[39;00m raw_speech]\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_speech, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 216\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_speech\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_speech, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m raw_speech\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mfloat64):\n\u001b[1;32m    218\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m raw_speech\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'dict'"
     ]
    }
   ],
   "source": [
    "feature_extractor(ds['train'], sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01dd4d3-2a83-4e44-afad-2210276816ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "inputs = feature_extractor(sample['audio']['array'], sampling_rate = sample['audio']['sampling_rate'])\n",
    "\n",
    "input_features = torch.Tensor(np.array(inputs.input_features))\n",
    "print(len(input_features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02ef7516-2d77-4483-81de-75c4d70efce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hebrew'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(input_features).logits\n",
    "\n",
    "predicted_class_ids = torch.argmax(logits).item()\n",
    "predicted_label = model.config.id2label[predicted_class_ids]\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7e30cba-6dae-446a-a156-b90f464d763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label = {i:i for i in range(6)}\n",
    "model.config.label2id = model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa36b07-4da9-4798-992f-e9d11469b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.num_labels = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b448d6a2-ff78-4e04-baf4-71c1352e84e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/mk8574/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Nov 22 21:05:41 2023) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "training_args = TrainingArguments(output_dir = \"test_trainer\", report_to = 'none')\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis = -1)\n",
    "    \n",
    "    return metric.compute(predictions = predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ec277f-5be9-439a-85dc-bea8ceb125b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = ds['train'][:1000]\n",
    "small_eval_dataset = ds['train'][1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "845e0593-8b42-4e97-a72e-c5424abf8609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = small_train_dataset,\n",
    "    eval_dataset = small_eval_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aa5d482-773e-4e7f-a2b9-2e84c66444e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForAudioClassification(\n",
       "  (encoder): WhisperEncoder(\n",
       "    (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (embed_positions): Embedding(1500, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x WhisperEncoderLayer(\n",
       "        (self_attn): WhisperAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projector): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (classifier): Linear(in_features=256, out_features=102, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f7b91df-cdf8-418d-832b-5e890f64137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor: AutoFeatureExtractor,\n",
    "        transforms: list = None,\n",
    "    ):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def to_dataset(self, df: pd.DataFrame) -> Dataset:\n",
    "        def load_waveform(row):\n",
    "            waveform, sample_rate = librosa.load(row[\"path\"])\n",
    "            waveform = librosa.resample(\n",
    "                waveform,\n",
    "                orig_sr=sample_rate,\n",
    "                target_sr=self.feature_extractor.sampling_rate,\n",
    "            )\n",
    "            row[\"waveform\"] = waveform\n",
    "\n",
    "            return row\n",
    "\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        dataset = dataset.map(load_waveform, num_proc=4)\n",
    "        if \"label\" in dataset.column_names:\n",
    "            dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def apply_transforms(self, dataset: Dataset) -> Dataset:\n",
    "        def apply_transforms(batch):\n",
    "            waveforms = [self.transforms(samples=np.array(waveform, dtype=np.float32), sample_rate=self.feature_extractor.sampling_rate) for waveform in batch[\"waveform\"]]\n",
    "            batch[\"waveform\"] = waveforms\n",
    "\n",
    "            return batch\n",
    "\n",
    "        if self.transforms:\n",
    "            dataset = dataset.with_transform(apply_transforms)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def collate_fn(self, batch: list) -> BatchFeature:\n",
    "        if hasattr(self.feature_extractor, \"nb_max_frames\"):\n",
    "            padding = \"max_length\"\n",
    "        else:\n",
    "            padding = \"longest\"\n",
    "\n",
    "        waveforms = [data[\"waveform\"] for data in batch]\n",
    "        model_inputs = self.feature_extractor(\n",
    "            waveforms,\n",
    "            sampling_rate=self.feature_extractor.sampling_rate,\n",
    "            padding=padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if \"labels\" in batch[0]:\n",
    "            labels = [data[\"labels\"] for data in batch]\n",
    "            model_inputs[\"labels\"] = torch.tensor(labels)\n",
    "\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21f2ffb5-91f1-452e-9821-f0079c33353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricScore(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        metrics = MetricCollection(\n",
    "            {\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=6),\n",
    "                \"recall\": MulticlassRecall(num_classes=6),\n",
    "                \"precision\": MulticlassPrecision(num_classes=6),\n",
    "                \"f1\": MulticlassF1Score(num_classes=6),\n",
    "            }\n",
    "        )\n",
    "        self.train_metrics = metrics.clone()\n",
    "        self.valid_metrics = metrics.clone()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "\n",
    "    def add_train_metrics(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        self.train_metrics.update(logits, labels)\n",
    "\n",
    "    def add_valid_metrics(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        self.valid_metrics.update(logits, labels)\n",
    "\n",
    "    def add_train_loss(self, loss: torch.Tensor):\n",
    "        self.train_losses.append(loss.item())\n",
    "\n",
    "    def add_valid_loss(self, loss: torch.Tensor):\n",
    "        self.valid_losses.append(loss.item())\n",
    "\n",
    "    def compute_train(self) -> Dict[str, Any]:\n",
    "        scores = self.train_metrics.compute()\n",
    "        for metric_key, score in scores.items():\n",
    "            if isinstance(score, torch.Tensor):\n",
    "                scores[metric_key] = score.item()\n",
    "        scores.update({\"loss\": np.mean(self.train_losses)})\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def compute_valid(self) -> Dict[str, Any]:\n",
    "        scores = self.valid_metrics.compute()\n",
    "        for metric_key, score in scores.items():\n",
    "            if isinstance(score, torch.Tensor):\n",
    "                scores[metric_key] = score.item()\n",
    "        scores.update({\"loss\": np.mean(self.valid_losses)})\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def reset(self):\n",
    "        self.train_metrics.reset()\n",
    "        self.valid_metrics.reset()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "\n",
    "    def print_summary(self, epoch_idx: int):\n",
    "        train_result = self.compute_train()\n",
    "        valid_result = self.compute_valid()\n",
    "\n",
    "        assert list(train_result.keys()) == list(valid_result.keys())\n",
    "\n",
    "        pt = PrettyTable()\n",
    "        pt.field_names = [f\"epoch {epoch_idx}\"] + list(train_result.keys())\n",
    "\n",
    "        train_row = [\"train\"]\n",
    "        for score in train_result.values():\n",
    "            train_row.append(round(score, 3))\n",
    "        pt.add_row(train_row)\n",
    "\n",
    "        valid_row = [\"valid\"]\n",
    "        for score in valid_result.values():\n",
    "            valid_row.append(round(score, 3))\n",
    "        pt.add_row(valid_row)\n",
    "\n",
    "        print(pt, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56c06a9b-376e-4868-8edb-8f1b96f5b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model: AutoModelForAudioClassification,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    max_epoch: int = 64,\n",
    "    lr: float = 5e-4,\n",
    "    gradient_accumulate_step: int = 1,\n",
    "    early_stop_patience: int = 5,\n",
    ") -> dict:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    metric_scores = MetricScore().to(device)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        [{\"params\": module.parameters(), \"lr\": lr if name == \"classifier\" else lr * 0.1} for name, module in model.named_children()],\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    # run finetune\n",
    "    best_score = 0.0\n",
    "    best_state_dict = None\n",
    "\n",
    "    early_stop_count = 0\n",
    "    for epoch_idx in range(1, max_epoch):\n",
    "        with torch.set_grad_enabled(True), tqdm(total=len(train_loader), desc=f\"[Epoch {epoch_idx}/{max_epoch}] training\", leave=False) as pbar:\n",
    "            model.train()\n",
    "            for step_idx, batch in enumerate(train_loader, 1):\n",
    "                batch = batch.to(device)\n",
    "                \n",
    "                # print(batch['input_features'].shape)\n",
    "                output = model(**batch)\n",
    "                loss = output.loss\n",
    "                loss.backward()\n",
    "\n",
    "                if step_idx % gradient_accumulate_step == 0 or step_idx == len(train_loader):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()            \n",
    "\n",
    "                metric_scores.add_train_loss(loss)\n",
    "                metric_scores.add_train_metrics(output.logits, batch.labels)\n",
    "\n",
    "                pbar.update()\n",
    "                pbar.set_postfix({\"train loss\": loss.item()})\n",
    "\n",
    "        with torch.set_grad_enabled(False), tqdm(total=len(valid_loader), desc=f\"[Epoch {epoch_idx}/{max_epoch}] validation\", leave=False) as pbar:\n",
    "            model.eval()\n",
    "            for batch in valid_loader:\n",
    "                batch = batch.to(device)\n",
    "\n",
    "                output = model(**batch)\n",
    "                loss = output.loss\n",
    "\n",
    "                metric_scores.add_valid_loss(loss)\n",
    "                metric_scores.add_valid_metrics(output.logits, batch.labels)\n",
    "\n",
    "                pbar.update()\n",
    "                pbar.set_postfix({\"valid loss\": loss.item()})\n",
    "\n",
    "        epoch_score = metric_scores.compute_valid()[\"accuracy\"]\n",
    "        metric_scores.print_summary(epoch_idx=epoch_idx)\n",
    "        metric_scores.reset()\n",
    "\n",
    "        lr_scheduler.step(epoch_score)\n",
    "\n",
    "        if epoch_score > best_score:\n",
    "            best_score = epoch_score\n",
    "            best_state_dict = model.state_dict()\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count == early_stop_patience:\n",
    "                print(\"*** EARLY STOPPED ***\")\n",
    "                break\n",
    "    \n",
    "    return best_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00aa444c-32e7-4642-b6b4-fdeb9dfd3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(\n",
    "    model: AutoModelForAudioClassification,\n",
    "    feature_extractor: AutoFeatureExtractor,\n",
    "    test_dataset: Dataset,\n",
    "    batch_size: int = 16,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predict_logits = {\"id\": []}\n",
    "    predict_logits.update({class_id: [] for class_id in range(6)})\n",
    "\n",
    "    predict_class = {\"id\": [], \"label\": []}\n",
    "\n",
    "    for batch_idx in tqdm(range(0, len(test_dataset), batch_size), desc=\"prediction\"):\n",
    "        bs, bi = batch_idx, batch_idx + batch_size\n",
    "        batch = test_dataset[bs:bi]\n",
    "\n",
    "        if hasattr(feature_extractor, \"nb_max_frames\"):\n",
    "            padding = \"max_length\"\n",
    "        else:\n",
    "            padding = \"longest\"\n",
    "\n",
    "        model_inputs = feature_extractor(\n",
    "            batch[\"waveform\"],\n",
    "            sampling_rate=feature_extractor.sampling_rate,\n",
    "            padding=padding,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        model_output = model(**model_inputs)\n",
    "\n",
    "        batch_logits = model_output.logits.cpu()\n",
    "        batch_predict = model_output.logits.argmax(dim=-1).cpu()\n",
    "\n",
    "        predict_logits[\"id\"] += batch[\"id\"]\n",
    "        for class_id in range(6):\n",
    "            predict_logits[class_id] += batch_logits[:, class_id].tolist()\n",
    "\n",
    "        predict_class[\"id\"] += batch[\"id\"]\n",
    "        predict_class[\"label\"] += batch_predict.tolist()\n",
    "\n",
    "    predict_logits = pd.DataFrame(predict_logits)\n",
    "    predict_class = pd.DataFrame(predict_class)\n",
    "\n",
    "    return predict_logits, predict_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e974c7a9-b41d-48e7-b3fa-2d1639dc8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = audiomentations.OneOf(\n",
    "    [\n",
    "        audiomentations.AddGaussianNoise(p=0.75),\n",
    "        audiomentations.PitchShift(p=0.75),\n",
    "        audiomentations.PeakingFilter(p=0.75),\n",
    "        audiomentations.SevenBandParametricEQ(p=0.75),\n",
    "        audiomentations.BandPassFilter(p=0.75),\n",
    "        audiomentations.BandStopFilter(p=0.75),\n",
    "        audiomentations.AirAbsorption(p=0.75),\n",
    "        audiomentations.ClippingDistortion(p=0.75),\n",
    "        audiomentations.HighPassFilter(p=0.75),\n",
    "        audiomentations.HighShelfFilter(p=0.75),\n",
    "        audiomentations.Limiter(p=0.75),\n",
    "        audiomentations.LowPassFilter(p=0.75),\n",
    "        audiomentations.LowShelfFilter(p=0.75),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36f49409-e57d-4650-ac97-468180e94e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train data and valid data\n",
    "df = pd.read_csv(config.train_csv)\n",
    "df[\"path\"] = df[\"path\"].apply(lambda x: os.path.join(os.path.dirname(config.train_csv), x[2:]))\n",
    "\n",
    "# create test data\n",
    "test_df = pd.read_csv(config.test_csv)\n",
    "test_df[\"path\"] = test_df[\"path\"].apply(lambda x: os.path.join(os.path.dirname(config.test_csv), x[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed398222-a85b-4bf7-ba2d-5d3cbe2217ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train data and valid data\n",
    "df = pd.read_csv(config.train_csv)\n",
    "df[\"path\"] = df[\"path\"].apply(lambda x: os.path.join(os.path.dirname(config.train_csv), x[2:]))\n",
    "\n",
    "if config.k_fold_num > 0:\n",
    "    skf = StratifiedKFold(n_splits=config.k_fold_num)\n",
    "    train_indices, valid_indices = list(skf.split(df, df[\"label\"]))[config.k_fold_idx]\n",
    "    train_df, valid_df = df.iloc[train_indices], df.iloc[valid_indices]\n",
    "else:\n",
    "    train_df, valid_df = train_test_split(df, train_size=0.9, stratify=df[\"label\"], random_state=seed)\n",
    "\n",
    "# create test data\n",
    "test_df = pd.read_csv(config.test_csv)\n",
    "test_df[\"path\"] = test_df[\"path\"].apply(lambda x: os.path.join(os.path.dirname(config.test_csv), x[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e99efdb3-a7d8-445f-a5c2-5872584849c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 501/501 [00:00<00:00, 558.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = DataModule(\n",
    "    feature_extractor=feature_extractor,\n",
    "    transforms=transforms,\n",
    ")\n",
    "train_dataset = data_module.apply_transforms(ds)\n",
    "valid_dataset = data_module.to_dataset(valid_df)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ds['train'], \n",
    "    # train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_module.collate_fn,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_module.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22507c1f-78bd-47cc-8c4d-56422ac2b55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 5001\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 1881\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d79d002-cfa2-4106-afd4-db1f059a2e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'label'],\n",
       "    num_rows: 5001\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07c8da3c-2b36-4eb9-a587-a9f17ecb19c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['test', 'train']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulate_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulate_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stop_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(best_state_dict)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39msave_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, train_loader, valid_loader, max_epoch, lr, gradient_accumulate_step, early_stop_patience)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m), tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] training\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     30\u001b[0m         batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# print(batch['input_features'].shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/datasets/dataset_dict.py:67\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     63\u001b[0m available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     64\u001b[0m     split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     65\u001b[0m ]\n\u001b[1;32m     66\u001b[0m suggested_split \u001b[38;5;241m=\u001b[39m available_suggested_splits[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m available_suggested_splits \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please first select a split. For example: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`my_dataset_dictionary[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggested_split\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m][\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable splits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['test', 'train']\""
     ]
    }
   ],
   "source": [
    "best_state_dict = fit(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    max_epoch=config.max_epoch,\n",
    "    lr=config.lr,\n",
    "    gradient_accumulate_step=config.gradient_accumulate_step,\n",
    "    early_stop_patience=config.early_stop_patience,\n",
    ")\n",
    "\n",
    "model.load_state_dict(best_state_dict)\n",
    "model.save_pretrained(os.path.join(config.save_dir, \"best_model\"))\n",
    "feature_extractor.save_pretrained(os.path.join(config.save_dir, \"best_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498783f2-b8fd-4925-9471-9d149eb4d54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2953d09-678d-4b7e-bb67-27eed28bd061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3860fba-c4d7-4b39-931d-ad7e3d92d014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98058c68-f3eb-43e4-bfd4-5febbc1637cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a30b2-6e78-4fed-8a22-3d4976d2e460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef0d75-c50c-413c-87b5-3bafa29b576a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78d477-de8b-496a-a57a-46370eb5fc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk8574_3.10 [~/.conda/envs/mk8574_3.10/]",
   "language": "python",
   "name": "conda_mk8574_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
