{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102cfb06-7c03-4990-96e1-05c841f85756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mk8574/.conda/envs/mk8574_3.10/lib/python3.10/site-packages/transformers/configuration_utils.py:381: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.weight', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'classifier.output.bias', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'classifier.output.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['projector.weight', 'classifier.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'classifier.bias', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"audio-classification\", model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d42072-41b3-47d3-8bc4-6eb3897aa3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/scratch/network/mk8574/audio_sentiment_challenge/data/train'\n",
    "test_path = '/scratch/network/mk8574/audio_sentiment_challenge/data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c988f60-f88e-47ef-ac6a-e006b0e49c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {'angry': 0, 'fearful': 1, 'sad': 2, 'disgust': 3, 'neutral': 4, 'happy': 5, 'surprised': -10, 'calm': 4}\n",
    "\n",
    "# emotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
    "\n",
    "# 0: angry\n",
    "# 1: fear\n",
    "# 2: sad\n",
    "# 3: disgust\n",
    "# 4: neutral\n",
    "# 5: happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9a58c-8cc1-496e-af66-d4508adee546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "def get_argmax(PATH):\n",
    "    res = pipe(PATH)\n",
    "    \n",
    "    index = 0\n",
    "    while transform[res[index]['label']] < 0:\n",
    "        index += 1\n",
    "\n",
    "    return transform[res[index]['label']]\n",
    "\n",
    "\n",
    "df = []\n",
    "\n",
    "for PATH in tqdm(glob(os.path.join(test_path, '*.wav'))):\n",
    "    label = get_argmax(PATH)\n",
    "    \n",
    "    name = os.path.splitext(os.path.basename(PATH))[0]\n",
    "    \n",
    "    df.append([name, label])\n",
    "    \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7f21c69-34fb-40a5-ae7a-241148de021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "df.insert(0, ['id', 'label'])\n",
    "\n",
    "with open('/scratch/network/mk8574/audio_sentiment_challenge/baseline_mk/submission.csv', mode = 'w', newline = '') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92d931-3e0b-4667-8c6b-4643a9acd632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk8574_3.10 [~/.conda/envs/mk8574_3.10/]",
   "language": "python",
   "name": "conda_mk8574_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
