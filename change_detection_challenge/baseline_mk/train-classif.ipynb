{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab49b1e6-4d5b-476f-9b68-74722b192e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train classifier \n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import numpy as np\n",
    "import shutil, random, os, sys, torch\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "prj_dir = '/scratch/network/mk8574/change_detection_challenge/baseline_mk'\n",
    "sys.path.append(prj_dir)\n",
    "# checkpoint_path = \"results/train/ ... /model.pt\"\n",
    "\n",
    "from modules.utils import load_yaml, get_logger\n",
    "from modules.metrics import get_metric_function\n",
    "from modules.earlystoppers import EarlyStopper\n",
    "from modules.losses import get_loss_function\n",
    "from modules.optimizers import get_optimizer\n",
    "from modules.schedulers import get_scheduler\n",
    "from modules.scalers import get_image_scaler\n",
    "from modules.datasets import SegDataset\n",
    "from modules.recorders import Recorder\n",
    "from modules.trainer import Trainer\n",
    "from models.utils import get_model\n",
    "\n",
    "def train():    \n",
    "    config_path = os.path.join(prj_dir, 'config', 'train.yaml')\n",
    "    config = load_yaml(config_path)\n",
    "    \n",
    "    # Set train serial: ex) 20211004\n",
    "    train_serial = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    train_serial = 'debug' if config['debug'] else train_serial\n",
    "\n",
    "    # Set random seed, deterministic\n",
    "    torch.cuda.manual_seed(config['seed'])\n",
    "    torch.manual_seed(config['seed'])\n",
    "    np.random.seed(config['seed'])\n",
    "    random.seed(config['seed'])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Set device(GPU/CPU)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(config['gpu_num'])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Create train result directory and set logger\n",
    "    train_result_dir = os.path.join(prj_dir, 'results', 'train', train_serial)\n",
    "    os.makedirs(train_result_dir, exist_ok=True)\n",
    "\n",
    "    # Set logger\n",
    "    logging_level = 'debug' if config['verbose'] else 'info'\n",
    "    logger = get_logger(name='train',\n",
    "                        file_path=os.path.join(train_result_dir, 'train.log'),\n",
    "                        level=logging_level)\n",
    "\n",
    "\n",
    "    # Set data directory\n",
    "    train_dirs = '../data/train' # os.path.join(prj_dir, 'data', 'train')\n",
    "\n",
    "    # Load data and create dataset for train \n",
    "    # Load image scaler\n",
    "    train_img_paths = glob(os.path.join(train_dirs, 'x', '*.png'))\n",
    "    train_img_paths, val_img_paths = train_test_split(train_img_paths, test_size=config['val_size'], random_state=config['seed'], shuffle=True)\n",
    "    \n",
    "    train_dataset = SegDataset(paths=train_img_paths,\n",
    "                            input_size=[config['input_width'], config['input_height']],\n",
    "                            scaler=get_image_scaler(config['scaler']),\n",
    "                            logger=logger)\n",
    "    val_dataset = SegDataset(paths=val_img_paths,\n",
    "                            input_size=[config['input_width'], config['input_height']],\n",
    "                            scaler=get_image_scaler(config['scaler']),\n",
    "                            logger=logger)\n",
    "    # Create data loader\n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                batch_size=config['batch_size'],\n",
    "                                num_workers=config['num_workers'], \n",
    "                                shuffle=config['shuffle'],\n",
    "                                drop_last=config['drop_last'])\n",
    "                                \n",
    "    val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                                batch_size=config['batch_size'],\n",
    "                                num_workers=config['num_workers'], \n",
    "                                shuffle=False,\n",
    "                                drop_last=config['drop_last'])\n",
    "\n",
    "    logger.info(f\"Load dataset, train: {len(train_dataset)}, val: {len(val_dataset)}\")\n",
    "    \n",
    "    # Load model:\n",
    "    model = Difference_Model().to(device)\n",
    "    # \"Load model architecture: {config['architecture']}\")\n",
    "\n",
    "    # Set optimizer\n",
    "    optimizer = get_optimizer(optimizer_str=config['optimizer']['name'])\n",
    "    optimizer = optimizer(model.parameters(), **config['optimizer']['args'])\n",
    "    \n",
    "    # Set Scheduler\n",
    "    scheduler = get_scheduler(scheduler_str=config['scheduler']['name'])\n",
    "    scheduler = scheduler(optimizer=optimizer, **config['scheduler']['args'])\n",
    "\n",
    "    # Set loss function\n",
    "    loss_func = get_loss_function(loss_function_str=config['loss']['name'])\n",
    "    loss_func = loss_func(**config['loss']['args'])\n",
    "\n",
    "    # Set metric\n",
    "    metric_funcs = {metric_name:get_metric_function(metric_name) for metric_name in config['metrics']}\n",
    "    logger.info(f\"Load optimizer:{config['optimizer']['name']}, scheduler: {config['scheduler']['name']}, loss: {config['loss']['name']}, metric: {config['metrics']}\")\n",
    "\n",
    "    # Set trainer\n",
    "    trainer = Trainer(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    loss_func=loss_func,\n",
    "                    metric_funcs=metric_funcs,\n",
    "                    device=device,\n",
    "                    logger=logger) #checkpoint_path = checkpoint_path\n",
    "    logger.info(f\"Load trainer\")\n",
    "\n",
    "    # Set early stopper\n",
    "    early_stopper = EarlyStopper(patience=config['earlystopping_patience'],\n",
    "                                logger=logger)\n",
    "    # Set recorder\n",
    "    recorder = Recorder(record_dir=train_result_dir, model=model, optimizer=optimizer, scheduler=scheduler, logger=logger)\n",
    "    logger.info(\"Load early stopper, recorder\")\n",
    "\n",
    "    # Recorder - save train config\n",
    "    shutil.copy(config_path, os.path.join(recorder.record_dir, 'train.yaml'))\n",
    "    \n",
    "    # (ADDED) Recorder - save validation paths\n",
    "    with open(os.path.join(train_result_dir, 'valids'), 'w') as f:\n",
    "        f.write(str(val_img_paths))\n",
    "    \n",
    "    # Train\n",
    "    print(\"START TRAINING\")\n",
    "    logger.info(\"START TRAINING\")\n",
    "    for epoch_id in range(config['n_epochs']):\n",
    "        \n",
    "        # Initiate result row\n",
    "        row = dict()\n",
    "        row['epoch_id'] = epoch_id\n",
    "        row['train_serial'] = train_serial\n",
    "        row['lr'] = trainer.scheduler.get_last_lr()\n",
    "\n",
    "        # Train\n",
    "        print(f\"Epoch {epoch_id}/{config['n_epochs']} Train..\")\n",
    "        logger.info(f\"Epoch {epoch_id}/{config['n_epochs']} Train..\")\n",
    "        tic = time()\n",
    "        trainer.train(dataloader=train_dataloader, epoch_index=epoch_id)\n",
    "        toc = time()\n",
    "        # Write train result to result row\n",
    "        row['train_loss'] = trainer.loss  # Loss\n",
    "        for metric_name, metric_score in trainer.scores.items():\n",
    "            row[f'train_{metric_name}'] = metric_score\n",
    "\n",
    "        row['train_elapsed_time'] = round(toc-tic, 1)\n",
    "        # Clear\n",
    "        trainer.clear_history()\n",
    "\n",
    "        # Validation\n",
    "        print(f\"Epoch {epoch_id}/{config['n_epochs']} Validation..\")\n",
    "        logger.info(f\"Epoch {epoch_id}/{config['n_epochs']} Validation..\")\n",
    "        tic = time()\n",
    "        trainer.validate(dataloader=val_dataloader, epoch_index=epoch_id)\n",
    "        toc = time()\n",
    "        row['val_loss'] = trainer.loss\n",
    "        # row[f\"val_{config['metric']}\"] = trainer.score\n",
    "        for metric_name, metric_score in trainer.scores.items():\n",
    "            row[f'val_{metric_name}'] = metric_score\n",
    "        row['val_elapsed_time'] = round(toc-tic, 1)\n",
    "        trainer.clear_history()\n",
    "\n",
    "        # Performance record - row\n",
    "        recorder.add_row(row)\n",
    "        \n",
    "        # Performance record - plot\n",
    "        recorder.save_plot(config['plot'])\n",
    "\n",
    "        # Check early stopping\n",
    "        early_stopper.check_early_stopping(row[config['earlystopping_target']])\n",
    "        if early_stopper.patience_counter == 0:\n",
    "            recorder.save_weight(epoch=epoch_id)\n",
    "            \n",
    "        if early_stopper.stop:\n",
    "            print(f\"Epoch {epoch_id}/{config['n_epochs']}, Stopped counter {early_stopper.patience_counter}/{config['earlystopping_patience']}\")\n",
    "            logger.info(f\"Epoch {epoch_id}/{config['n_epochs']}, Stopped counter {early_stopper.patience_counter}/{config['earlystopping_patience']}\")\n",
    "            break\n",
    "               \n",
    "    \n",
    "    print(\"END TRAINING\")\n",
    "    logger.info(\"END TRAINING\")\n",
    "    \n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def decompose(x):\n",
    "    width, height = x.size\n",
    "    half_width = width // 2\n",
    "\n",
    "    left_half = x.crop((0, 0, half_width, height))\n",
    "    right_half = x.crop((half_width, 0, width, height))\n",
    "\n",
    "    return left_half, right_half\n",
    "    \n",
    "class Difference_Model(nn.Module):       \n",
    "    def __init__(self):\n",
    "        super(Difference_Model, self).__init__()\n",
    "        config_path = os.path.join('./config', 'train.yaml')\n",
    "        config = load_yaml(config_path)\n",
    "\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(config['gpu_num'])\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        \n",
    "        model = smp.DeepLabV3Plus(classes=config['n_classes'],\n",
    "                encoder_name=config['encoder'],\n",
    "                encoder_weights=config['encoder_weight'],\n",
    "                activation=config['activation']).to(device)\n",
    "        self.encoder = model.encoder\n",
    "        self.decoder = model.decoder\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # split x into two\n",
    "        a, b = decompose(x)\n",
    "        # process as input\n",
    "        a = self.encoder(a)\n",
    "        b = self.encoder(b)\n",
    "\n",
    "        # compute differences\n",
    "        res = a - b\n",
    "\n",
    "        # process as output\n",
    "        return self.decoder(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f986d1-04a3-4a5e-bd81-ff6a82981463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             y_left = y[:,:self.input_size[0]/2]\n",
    "#             y_right = y[:,self.input_size[0]/2:]\n",
    "            \n",
    "#             y = y_left + y_right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8696ef3f-dc18-487f-9c6e-9ad0b9b144ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 4])\n",
      "torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from skimage import color\n",
    "from skimage import morphology\n",
    "\n",
    "\n",
    "def resize_tensor(tensor):\n",
    "    tensor = torch.mean(tensor, 1)\n",
    "    tensor = tensor.squeeze()\n",
    "    tensor = torch.mean(tensor, 1)\n",
    "    tensor = tensor.permute(0, 1)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    return tensor\n",
    "\n",
    "tensor = torch.randn((2, 3, 4, 4))\n",
    "print(tensor.shape)\n",
    "tensor = resize_tensor(tensor)\n",
    "print(tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aa5b0b-a79d-4a9b-9e59-2ee3feb534e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk8574_3.10 [~/.conda/envs/mk8574_3.10/]",
   "language": "python",
   "name": "conda_mk8574_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
